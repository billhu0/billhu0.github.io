<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Bill Hu"><meta name="keywords" content=""><meta name="description" content="CMU 15-618 Notes"><meta property="og:type" content="article"><meta property="og:title" content="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"><meta property="og:url" content="https://www.billhu.us/2025/054_cmu_15618/index.html"><meta property="og:site_name" content="Bill Hu&#39;s Blog"><meta property="og:description" content="CMU 15-618 Notes"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/ispc-sinx-interleave.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/hw-shared-address.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/NUMA.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/gather.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/parallel-steps.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/synchronous-send-recv.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/async-send-recv.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/pipelined-communication.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/change-grid-traversal-order.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/fusing-loops.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/sharing-data.png"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/4d-array.png"><meta property="article:published_time" content="2025-01-07T19:17:06.000Z"><meta property="article:modified_time" content="2025-01-31T22:23:40.521Z"><meta property="article:author" content="Bill Hu"><meta property="article:tag" content="C"><meta property="article:tag" content="Lecture Notes"><meta property="article:tag" content="CUDA"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.png"><title>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming - Bill Hu&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.15.6/katex.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/myfont.css"><link rel="stylesheet" href="/css/jetbrains-mono.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"www.billhu.us",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:25,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script>!function(t,e,n,c,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/fjnxcr4gva",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta name="google-site-verification" content="IqNfw3GiMxuhw7kYoEdhMJoh3j99KHuGI9bw00hPn2c"><link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet"><meta name="generator" content="Hexo 7.1.1"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Bill Hu&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>Home</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>Archives</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>Categories</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>Tags</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>About</span></a></li><li class="nav-item"><a class="nav-link" href="https://www.billhu.xyz" target="_self"><i class="iconfont icon-code"></i> <span>Portfolio</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-01-07 19:17" pubdate>January 7, 2025 pm</time></span></div><div class="mt-1"></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</h1><div class="markdown-body"><h1 id="lec-2-a-modern-multi-core-processor"><a class="markdownIt-Anchor" href="#lec-2-a-modern-multi-core-processor"></a> Lec 2. A modern multi-core processor</h1><p>4 key concepts: 两个与parallel execution有关, 两个与challenges of accessing memory 有关</p><h2 id="parallel-execution"><a class="markdownIt-Anchor" href="#parallel-execution"></a> Parallel execution</h2><ul><li><strong>Superscalar processor</strong>: Instruction level parallelism (ILP)<ul><li>ILP读未来的指令（每个周期读两条指令），有两个fetch/decode单元和两个exec单元，能够同时执行两条指令</li></ul></li><li><strong>Multi-core</strong>: 多个processing cores<ul><li>多核之前，处理器提升重点在更大缓存，更多分支预测predictor等；同时更多晶体管（才能放得下更多缓存和更多predictor和乱序执行逻辑）促生更小的晶体管，促进更高的计算机主频</li><li>2004年多核出现之后，人们在一个chip上放多个processor，用更多晶体管放更多核心。</li></ul></li><li><strong>SIMD processing</strong> (aka <strong>Vector processing</strong>): 多个ALU(同一个core内)<ul><li>仍然只需要一个fetch/decode单元，多个ALU。</li><li>conditional execution: 如果想simd的程序块有if else，要通过mask处理</li><li>手写avx代码（cpu指令）是<strong>explicit SIMD</strong>; 而GPU是<strong>implicit SIMD</strong>，因为compiler生成的并不是并行指令（是普通的scalar instructions），只有GPU硬件运行才是SIMD的</li></ul></li></ul><h2 id="accessing-memory"><a class="markdownIt-Anchor" href="#accessing-memory"></a> Accessing memory</h2><ul><li><p><strong>cache</strong>: <strong>reduce latency</strong></p></li><li><p><strong>prefetching</strong> reduces stalls: <strong>hides latency</strong></p></li><li><p><strong>Multi-threading</strong>, <strong>interleave</strong> processing of multiple threads</p><ul><li>跟prefetching一样，也是hide latency，不能reduce latency</li><li>指的是：开多个线程，在一个线程卡住的时候执行别的线程</li><li>在下图中，创建thread1的时候不仅仅创建thread1，还会告诉电脑创建了thread 2 3 4，硬件检测线程是否发生了stall（被等待内存操作卡住），如果发生了stall会很快切换到别的线程，想juggling一样。硬件决定如何juggle这些线程。</li><li>这样memory latency仍然存在，但是被hide了。memory latency在后台发生，前台CPU一直在执行有用的工作。</li><li>这种操作会导致单个线程的执行时间变长（因为thread1从runnable到重新开始执行有一段空挡（这段空隙在执行thread 2 3 4）。</li><li>需要更多硬件资源，存储线程的register等状态信息，这样切换线程才会快。且需要较大的memory bandwidth。</li></ul></li></ul><img src="/2025/054_cmu_15618/interleave-thread.png" srcset="/img/loading.gif" lazyload><p>GPU设计成处理大量数据（远大于核内缓存的数据量）。</p><p>与CPU内存相比，GPU显存带宽更高，但延迟更高。</p><h1 id="lec-3-programming-models"><a class="markdownIt-Anchor" href="#lec-3-programming-models"></a> Lec 3. Programming Models</h1><h2 id="abstraction-vs-implementation"><a class="markdownIt-Anchor" href="#abstraction-vs-implementation"></a> Abstraction vs Implementation</h2><p>abstraction和implementation不是一个东西！</p><p><strong>ISPC</strong> (Intel SPMD Program Compiler)</p><p>SPMD: single program multiple data</p><p>一个ISPC计算sin(x)的例子</p><ul><li><p>Interleaved</p><ul><li><img src="/2025/054_cmu_15618/ispc-sinx-interleave.png" srcset="/img/loading.gif" lazyload></li><li><pre class="highlight"><code class="c">export <span class="hljs-type">void</span> <span class="hljs-title function_">sinx</span><span class="hljs-params">(
  uniform <span class="hljs-type">int</span> N, uniform <span class="hljs-type">int</span> terms,
  uniform <span class="hljs-type">float</span>* x, uniform <span class="hljs-type">float</span>* result)</span> &#123;
  <span class="hljs-comment">// assume N % programCount = 0</span>
  <span class="hljs-keyword">for</span> (uniform <span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;N; i+=programCount) &#123;
    <span class="hljs-type">int</span> idx = i + programIndex;
    <span class="hljs-comment">// 不重要</span>
  &#125;
&#125;
&lt;!--code￼<span class="hljs-number">0</span>--&gt;

</code></pre></li></ul></li></ul><p>在这个示例中blocked比interleaved更好。</p><p>因为每个iteration工作量完全相同，SIMD指令load连续内存（直接_mm_load_ps1）比load不连续内存(这种操作叫gather，只在AVX2及以后才支持）更快。</p><p>我们可以使用<code>foreach</code>来代替。</p><p><code>foreach</code>表示循环中的每一次iteration都是独立的，由ISPC决定如何分配</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">foreach(i = <span class="hljs-number">0</span> ... N) &#123;<br>	<span class="hljs-comment">// index ...</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>ISPC: abstraction VS. Implementation<ul><li>Programming model: SPMD<ul><li>程序员想的是，我们有programCount个逻辑指令流，写代码也按照这样的abstration去写</li></ul></li><li>Implementation: SIMD<ul><li>ISPC输出SSE4或AVX这种指令来实现逻辑操作</li></ul></li></ul></li></ul><h2 id="four-programming-models"><a class="markdownIt-Anchor" href="#four-programming-models"></a> Four Programming Models</h2><ul><li><p><strong>Shared Address Space</strong> model</p><ul><li><p>共享内存，不同线程通过读写同一块内存来通信</p></li><li><p>很多人认为这是最方便易用的model，因为这和sequential programming最相近</p></li><li><p>每个处理器都能访问任意内存地址</p></li><li><p><strong>Uniform memory access</strong> time; Symmetric shared-memory multi-processor (SMP), : 指每个处理器都能访问内存且访问内存所需时间一致。<strong>Scalability不好。</strong></p><img src="/2025/054_cmu_15618/hw-shared-address.png" srcset="/img/loading.gif" lazyload></li><li><p><strong>Non-uniform memory access (NUMA)</strong>: 每个处理器都能访问完整内存，但所需时间不一致。这样<strong>Scalability</strong>比较好。但是performance tuning可能需要更多精力。<img src="/2025/054_cmu_15618/NUMA.png" srcset="/img/loading.gif" lazyload></p></li></ul></li><li><p><strong>Message Passing</strong> model</p><ul><li>线程之间不共享内存（address space不共享），只能通过发送和接收messages通信</li><li>相比shared memory的优点：不需要别的硬件，可以纯软件做，实现起来简单。常用的库是<strong>MPI</strong> (message passing interface)</li><li>现代很常用的操作是，在一个节点（节点内是多核的）内用shared address，在不同节点间用message passing</li></ul></li></ul><div class="note note-info"><p>这里很搞：abstraction can target different types of machines.</p><p>分清<strong>abstraction</strong>和<strong>implementation</strong>的区别！</p><p>比如说：</p><p>message passing 这个 <strong>abstraction</strong> 可以使用硬件上的 shared address space 来 <strong>implement</strong>。</p><ul><li>发送/接收消息就是读写message library的buffer。</li></ul><p>shared address space 这个 <strong>abstraction</strong> 在不支持硬件 shared address space 的机器上也可以用软件来 <strong>implement</strong> (但是低效)</p><ul><li>所有涉及共享变量的page都标记成invalid，然后使用page-fault handler来处理网络请求</li></ul></div><ul><li><p><strong>The data-parallel</strong> model</p><ul><li>上面的shared address space和message passing更general</li><li>data-parallel更specialized, rigid。</li><li>在不同的数据（数据块）上执行相同的操作。</li><li>通常用SPMD的形式：<strong>map (function, collection)</strong>，其中对所有的数据都做function的操作，function可以是很长的一段逻辑，比如一个loop body。collection是一组数据，可以包含多个数据。</li><li><strong>gather/scatter</strong>: gather是把本来不连续的数据按照index放到一起，scatter是把本来连续的数据分散开。<img src="/2025/054_cmu_15618/gather.png" srcset="/img/loading.gif" lazyload></li></ul></li><li><p><strong>The systolic arrays</strong> model</p><ul><li><p>读内存太慢了，memory bandwidth会成为瓶颈</p></li><li><p>所以要<strong>避免不必要的内存读取</strong>，尽量只读一次内存，做完所有需要用到这块内存的操作，再写回去。</p></li><li><p>示例：矩阵乘法 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2VrnkXd9QR8">https://www.youtube.com/watch?v=2VrnkXd9QR8</a></p></li></ul></li></ul><h1 id="lec-5-parallel-programming-basics"><a class="markdownIt-Anchor" href="#lec-5-parallel-programming-basics"></a> Lec 5. Parallel Programming Basics</h1><img src="/2025/054_cmu_15618/parallel-steps.png" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Decomposition</strong></p><p>把问题分解成 <strong>tasks</strong></p><p>Main idea: 创造至少能把机器占满的tasks数量，通常对于t个processor会给多于t个task，并且要让这些task尽可能 <strong>independent</strong>.</p><p><strong>Amdahl’s Law</strong>: 程序中有S部分只能顺序运行（无法用并行加速）则整个程序的speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">\leq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7719400000000001em;vertical-align:-.13597em"></span><span class="mrel">≤</span></span></span></span> 1/S.</p><p>通常是programmer负责decomposition。</p></li><li><p><strong>Assignment</strong></p><p>Goal: <strong>balance workload, reduce communication costs</strong></p><p>can be performed <strong>statically</strong> or <strong>dynamically</strong></p><ul><li>statically: e.g. ISPC <code>foreach</code></li><li>dynamically: e.g. ISPC <code>launch tasks</code> 运行的时候会维护线程池，线程池中的线程从任务队列中读。这样做的优点是runtime workload balance.</li></ul></li><li><p><strong>Orchestration</strong></p><p>Goal: <strong>reduce communication/sync cost</strong>, preserve locality of data reference, reduce overhead</p><p>需要考虑机器的特性（上面的decomposition和assignment不用太考虑）。</p><p>包括</p><ul><li><strong>structuring communication</strong>: 信息传递模型 e.g. 传一个chunk数据而不是只传一个byte，节约overhead</li><li><strong>adding synchronization</strong> to preserve dependencies</li><li>organizing <strong>data structures</strong> in memory</li><li><strong>scheduling</strong> tasks</li></ul></li><li><p><strong>Mapping</strong> to hardware</p><p>对程序员来说是optional的。programmer可以显式制定哪个thread跑在哪个processor上。</p><ul><li>mapping by <strong>OS</strong>: e.g. pthread</li><li>mapping by <strong>compiler</strong>: e.g. ISPC maps ISPC program instances to vector instruction lanes</li><li>mapping by <strong>hardware</strong> e.g. GPU map CUDA threads to GPU cores</li></ul><p>Mapping 还能有不同的decisions，比如说</p><ul><li>Place <strong>related</strong> threads on the same processor: 最大化locality，共享数据，减少通讯成本</li><li>Place <strong>unrelated</strong> threads on the same processor: 可能一个thread受制于内存带宽，另一个受制于计算时间，这两个thread放在一起可以让处理器利用率更高</li></ul></li></ul><p>A parallel programming example: 2D-grid based solver</p><p>TODO here.</p><h1 id="lec-6-work-distribution-scheduling"><a class="markdownIt-Anchor" href="#lec-6-work-distribution-scheduling"></a> Lec 6. Work Distribution &amp; Scheduling</h1><p>key goals:</p><ul><li>balance workload</li><li>reduce communication</li><li>reduce extra work (overhead)</li></ul><h2 id="workload-balance"><a class="markdownIt-Anchor" href="#workload-balance"></a> workload balance</h2><ul><li><p><strong>Static assignment</strong></p><p>任务分配在运行之前就已经 <strong>pre-determined</strong></p><p>例如之前讲的blocked assignment和interleaved assignment.</p><p><strong>Zero runtime overhead</strong></p><p>当任务量可预测时可以使用。不一定要任务量相同，可预测不会变就行。</p></li><li><p><strong>Semi-static assignment</strong></p><p>可预测未来短期内的任务量</p><p>一边运行一边profile并调整任务分配（periodically profiles itself and re-adjusts assignment）</p></li><li><p><strong>Dynamic assignment</strong></p><p>任务量unpredictable.</p><ul><li><pre class="highlight"><code class="c"><span class="hljs-keyword">while</span> (<span class="hljs-number">1</span>) &#123;
  <span class="hljs-type">int</span> i;
  lock(counter_lock);
  i = counter++;
  unlock(counter_lock);  <span class="hljs-comment">// 或使用 atomic_incr(counter); 代替</span>
  <span class="hljs-keyword">if</span> (i &gt;= N) <span class="hljs-keyword">break</span>;
  <span class="hljs-comment">// do with index i</span>
&#125;
&lt;!--code￼<span class="hljs-number">2</span>--&gt;

</code></pre></li></ul></li></ul><h3 id="scheuling-fork-join-programs"><a class="markdownIt-Anchor" href="#scheuling-fork-join-programs"></a> Scheuling fork-join programs</h3><ul><li><p>Bad idea: <code>cilk_spawn</code> --&gt; <code>pthread_create</code>, <code>cilk_sync</code> --&gt; <code>pthread_join</code></p><p>因为创建kernel thread开销很大。</p><p>应该用线程池。</p></li><li><p>让idle thread 从别家thread的queue里steal work to do.</p><p><strong>continuation first</strong>:</p><ul><li>record child for later execution</li><li>child is made available for stealing by other threads (child stealing)</li><li>在遇到spawn的时候，自己执行spawn后面的任务，并把spawn出来的放在自己的work queue里，等待别的线程（如果别的线程有空闲）steal自己的任务。</li><li>如果没有stealing，那么（相比于去除所有spawn语句）执行顺序全都是反的</li></ul><p><strong>child first</strong>:</p><ul><li>record continuation for later execution</li><li>continuation is made available for stealing by other threads (continuation stealing)</li><li>遇到spawn的时候，只创建一个可被steal的项目。</li></ul></li><li><p>work queue可以用<strong>dequeue</strong> (double-ended queue)实现</p><p>每一个线程有自己的work queue，针对自己的work queue，在尾部添加，从尾部取出</p><p>如果要steal别的线程的work queue，从头部取出</p></li></ul><h1 id="lec-7-locality-communication-and-contention"><a class="markdownIt-Anchor" href="#lec-7-locality-communication-and-contention"></a> Lec 7. Locality, Communication, and Contention</h1><p>Lec6讲如何平均分配任务，Lec7讲如何降低communication开销.</p><ul><li><p><strong>synchronous (blocking)</strong> send and receive</p><img src="/2025/054_cmu_15618/synchronous-send-recv.png" srcset="/img/loading.gif" lazyload></li><li><p><strong>non-blocking asynchronous</strong> send and receive</p></li></ul><p>send()和recv()函数会立即返回 在后台做事</p><img src="/2025/054_cmu_15618/async-send-recv.png" srcset="/img/loading.gif" lazyload><h2 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h2><p>使用Pipeline: <strong>Latency 不变, Throughput 增加</strong></p><p>例子：</p><ul><li><p>Communication = Overhead(橙色) + Occupancy (蓝色) + Network delay (灰色)</p></li><li><p>最长的部分是瓶颈，决定了throughput上限</p></li></ul><img src="/2025/054_cmu_15618/pipelined-communication.png" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Overlap</strong>: communication和其它工作同时运行的时间。</p><p>我们希望能尽可能增加overlap这样communication cost才会降低。</p><p>降低overlap的方法</p><ul><li>Example 1: <strong>Asynchronous</strong> message send/recv 异步消息</li><li>Example 2: <strong>Pipelining</strong> 发送多条消息时让这个发送过程overlap</li></ul></li></ul><h2 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h2><p>Communication包含inherent和artifactual</p><ul><li><p><strong>Inherent communication</strong>: 程序算法写好的，必须发生的通信</p><ul><li>Communication-to-computation ratio: 通信量/计算量 的比值。越低越好。</li><li><strong>arithmetic intensity</strong>: 1/communication-to-computation ratio. 越高越好。</li></ul></li><li><p><strong>Artifactual communication</strong>: 所有别的通信，因为memory hierarchy导致额外的通信，例如L1/L2/L3/内存/网络之间的通信。包括：</p><p>① 系统有<strong>minimum granularity of transfer</strong>: 即使只需要读取4byte数据，也需要复制64-byte整条cache line</p><p>② 系统有<strong>rules of operation</strong>: 例如，写入内存需要先把内存读到cache line中（write-allocate）之后踢出cache line再写入内存，导致一次写入操作需要访问两次内存</p><p>③ <strong>Poor placement</strong> of data in distributed memories: 被某个processor访问最多的数据并没有放在这个processor附近</p><p>④ Finite replication <strong>capacity</strong>: 因为cache太小放不下，会被踢掉，所以有一些数据频繁被踢出/放入cache</p><p>提高<strong>locality</strong>对降低artifactual communication很重要</p></li></ul><p>提高<strong>temporal locality</strong>的例子</p><ul><li><p>by changing grid traversal order</p><img src="/2025/054_cmu_15618/change-grid-traversal-order.png" srcset="/img/loading.gif" lazyload></li><li><p>by fusing loops</p><img src="/2025/054_cmu_15618/fusing-loops.png" srcset="/img/loading.gif" lazyload></li><li><p>by sharing data</p><img src="/2025/054_cmu_15618/sharing-data.png" srcset="/img/loading.gif" lazyload></li></ul><p>提高<strong>spatial locality</strong></p><ul><li><p>false sharing 不好</p></li><li><p><strong>4D array layout</strong> (<strong>blocked data layout</strong>): Embedding a 2D array within another 2D array allows page granularities to remain within a tile, making it practical to map data to local portions of physical memory (thereby reducing cache miss latencies to main memory).</p><img src="/2025/054_cmu_15618/4d-array.png" srcset="/img/loading.gif" lazyload></li></ul><h2 id="contention"><a class="markdownIt-Anchor" href="#contention"></a> Contention</h2><p><strong>Contention</strong>: 在短时间内很多人请求同一个resource</p><p>Example: distributed work queues (让每个线程有自己的work queue)可以降低contention</p><h2 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h2><ul><li>降低communication costs<ul><li><strong>Reduce overhead</strong>: 发更少的消息数量，更长的消息内容（合并短消息）</li><li><strong>Reduce delay</strong>：提高locality</li><li><strong>Reduce contention</strong>: 把contended resource分开，例如local copies, fine-grained locks</li><li><strong>Increase overlap</strong>: 用异步消息、pipeline等 提高communication和computation的overlap</li></ul></li></ul><h1 id="一些基础知识"><a class="markdownIt-Anchor" href="#一些基础知识"></a> 一些基础知识</h1><h2 id="ispc"><a class="markdownIt-Anchor" href="#ispc"></a> ISPC</h2><p>ISPC代码调用时会生成多个program instances, 可以利用 <code>programCount</code> 和 <code>programIndex</code> 来获取instance总数和当前instance编号。</p><p><code>uniform</code> 表示在一个SIMD程序块中，变量对所有SIMD通道都是相同的值。仅仅是一种优化，不影响正确性(因为uniform变量只需要加载一次或执行一次，编译器可以做出优化，不加uniform可能造成不必要的重复计算)。</p><p>非uniform (<code>varying</code>) 表示变量在不同SIMD通道可能有不同的值。</p><p>所以说 <code>programCount</code> 是 uniform, <code>programIndex</code> 是 varying.</p><hr><p>ISPC可以通过tasks来实现多核加速，利用多线程。</p><p>Contrary to threads, tasks do not have execution context and they are only pieces of work. ISPC编译器接受tasks并自行决定启动多少个threads。</p><p>通常我们应该启动比cpu逻辑线程数更多的tasks数量，但也不要太多，否则会有scheduling的overhead。</p><p>task自带 <code>taskIndex</code>。</p><h2 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h2><p>host是CPU, device是GPU</p><p><code>__device__</code>: 在device上执行，只能在device中调用</p><p><code>__global__</code>: 在device上执行，只能在host中调用</p><p><code>__host__</code>: 在host上执行且只能在host上调用</p><p><code>cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost)</code></p><hr><p>threads grouped into blocks</p><p>需要指明blocks的数量，和每个block中threads的数量。</p><p>假设n是总的threads数量, t是每个block中threads的数量。</p><p><code>KernelFunction&lt;&lt;&lt;ceil(n/t), t&gt;&gt;&gt;(args)</code></p><p>每一个thread都会运行同样的kernel，每一个thread由blockID和这个block中的threadID来标识。</p><hr><p>Example:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">vecAddKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> i = threadId.x + blockDim.x * blockId.x;<br>    <span class="hljs-keyword">if</span> (i&lt;n) C[i] = A[i] + B[i];<br>&#125;<br><span class="hljs-type">void</span> <span class="hljs-title function_">vecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> size = n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-type">float</span> *d_A, *d_B, *d_C;<br>  <br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_A, size);<br>    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_B, size);<br>    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_C, size);<br>  <br>    vecAddKernel&lt;&lt;&lt;<span class="hljs-built_in">ceil</span>(n/<span class="hljs-number">256</span>), <span class="hljs-number">256</span>&gt;&gt;&gt;(d_A, d_B, d_C, n);<br>    <br>  	cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);<br>    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);<br>&#125;<br></code></pre></td></tr></table></figure><p>注: 为什么<code>cudaMalloc</code>第一个参数是二级指针，而不直接使用返回值来赋值给指针？</p><p>因为 <code>cudaMalloc</code> 的返回值已经用来返回 <code>cudaError_t</code>。</p><hr><p>grid和blocks可以是1D, 2D, 3D的。上面这个例子是1D，所以是&quot;<code>.x</code>&quot;</p><p>2D的例子：假设要把一个WIDTH x WIDTH的矩阵P分成几块。</p><p>WIDTH=8, TILE_WIDTH为2的话，就是把8x8的矩阵分成16个小块(grid)，每一个小块大小是2x2(4个thread)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">dim3 <span class="hljs-title function_">dimGrid</span><span class="hljs-params">(WIDTH / TILE_WIDTH, WIDTH / TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>dim3 <span class="hljs-title function_">dimBlock</span><span class="hljs-params">(TILE_WIDTH, TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>MatrixMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(args);<br></code></pre></td></tr></table></figure><p>每一个thread可以用以下方式来标识</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">row    = blockId.y * blockDim.y + threadId.y;<br>column = blockId.x * blockDim.x + threadId.x;<br></code></pre></td></tr></table></figure><hr><p>为什么用两层threads？因为组成多个grid的thread blocks比一个很大的单个thread block更好管理。</p><p>GPU有很多很多核心，核心group成SM(streaming multiprocessors)，每一组SM有自己的内存和调度。</p><p>GPU不同时启动所有100万个threads，而是把大约1000个thread装进一个block里，并分发给SM。</p><p>assign给SM的thread block会使用SM的资源（寄存器和共享内存）。这些资源已经pre-allocated，且由于寄存器数量很多，在切换threads时不需要register flush。</p><hr><p>不同的block可以用任何顺序运行，因此不能assume block2在block1之后运行。如果真的要这么做，需要放在不同的kernel里（启动kernel比较耗资源）</p><p>同一个block中的thread可以使用 <code>__syncthreads()</code> 来做barrier synchronization。</p><p>但是通常不建议使用 <code>__syncthreads()</code></p><hr><p>如何选择合适的block size？</p><ul><li>Consideration 1: hardware constraints<ul><li>例如：每一个SM分配小于1536个thread，小于8个block；每一个block小于512个thread</li></ul></li><li>Consideration 2: complexity of each thread</li><li>Consideration 3: thread work imbalance.</li></ul><hr><p>GPU memory</p><p>Global memory很慢，所以同时运行大量线程，线程因为内存IO卡住的时候切换其它线程，这是massive multi-threading (MMT).</p><p>这样总的throughput很高，即使每个thread的延迟也很高。</p><p>每个SM有自己的scheduler，每个SM存储了所有thread的context(PC, reg等)，所以SM内能做到零开销线程切换。同时，SM scheduler有一个scoreboard追踪哪些thread是blocked/unblocked，所以SM有大约30个核但可以运行大约1000个线程。</p><hr><p>Tiled MM是一种进行矩阵乘法 内存友好的方法。</p><p>CUDA类型关键词</p><ul><li><code>__device__ __shared__</code> memory: shared; scope: block; lifetime: block</li><li><code>__device__</code> memory: global; scope: grid; lifetime: application</li><li><code>__device__ __constant__</code> memory: constant; scope: grid; lifetime: application</li></ul><hr><p>Race conditions:</p><p>CUDA中难以实现mutex，而且包含critical sections的代码在GPU上本来就运行得不好。</p><p>CUDA中有一些原子操作，可以在global或shared memory变量上操作</p><ul><li><code>int atomicInc(int *addr)</code>: 加一，返回旧值</li><li><code>int atomicAdd(int *addr, int val)</code>: 加val, 返回旧值</li><li><code>int atomicMax(int *addr, int val)</code>: 让*addr=max(*addr, val) 并返回旧值</li><li><code>int atomicExch(int *addr1, int val)</code>: set</li><li><code>int atomicCAS(int *addr, old, new)</code>: Compare and swap.<ul><li><code>if (*addr == old) *addr = new;</code></li></ul></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Lecture-Notes/" class="category-chain-item">Lecture Notes</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/C/" class="print-no-link">#C</a> <a href="/tags/Lecture-Notes/" class="print-no-link">#Lecture Notes</a> <a href="/tags/CUDA/" class="print-no-link">#CUDA</a></div></div><div class="license-box my-3"><div class="license-title"><div>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</div><div>https://www.billhu.us/2025/054_cmu_15618/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>Bill Hu</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>January 7, 2025</div></div><div class="license-meta-item"><div>Licensed under</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/057_cmu_17514/" title="[Lecture Notes] CMU 17-514 Software Construction"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">[Lecture Notes] CMU 17-514 Software Construction</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2024/051-cmu-15513/" title="[Lecture Notes] CMU 15-513 Intro to Computer Systems"><span class="hidden-mobile">[Lecture Notes] CMU 15-513 Intro to Computer Systems</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Table of Contents</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <span>With </span><a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-love"></i> <a href="/about" target="_blank" rel="nofollow noopener"><span>Bill Hu</span></a><div style="font-size:.85rem"><span id="timeDate">Loading days...</span> <span id="times">Loading time...</span><script src="/js/duration.min.js"></script></div></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,t=t.getElementById("subtitle");t&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript></body></html>