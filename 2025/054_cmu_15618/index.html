<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Bill Hu"><meta name="keywords" content=""><meta name="description" content="CMU 15-618 Notes"><meta property="og:type" content="article"><meta property="og:title" content="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"><meta property="og:url" content="https://www.billhu.us/2025/054_cmu_15618/index.html"><meta property="og:site_name" content="Bill Hu&#39;s Blog"><meta property="og:description" content="CMU 15-618 Notes"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/ispc-sinx-interleave.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/hw-shared-address.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/NUMA.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/gather.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/parallel-steps.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/synchronous-send-recv.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/async-send-recv.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/pipelined-communication.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/change-grid-traversal-order.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/fusing-loops.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/sharing-data.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/4d-array.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/interconnect-terminology.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/direct-indirect-networks.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/topology-1.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/topology-2.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/write-through-invalidation.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/msi.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/MESI.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/MOESIF.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/multilevel-cache-inclusion.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/nvgpu-no-cache-coherence.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/simple-directories.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg1.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg2.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg3.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/sparse-directories.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/mem-consistency-interleave.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/heterogeneity-r-example.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/heterogeneity-example-1.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/asymmetric-cores.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/asymmetric-speedup.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/dsl-triangle.webp"><meta property="article:published_time" content="2025-01-07T19:17:06.000Z"><meta property="article:modified_time" content="2025-11-04T02:51:49.437Z"><meta property="article:author" content="Bill Hu"><meta property="article:tag" content="C"><meta property="article:tag" content="Lecture Notes"><meta property="article:tag" content="CUDA"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.webp"><title>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming - Bill Hu&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.15.6/katex.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/myfont.css"><link rel="stylesheet" href="/css/jetbrains-mono.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"www.billhu.us",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:25,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script>!function(t,e,n,c,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/fjnxcr4gva",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta name="google-site-verification" content="IqNfw3GiMxuhw7kYoEdhMJoh3j99KHuGI9bw00hPn2c"><link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet"><meta name="generator" content="Hexo 7.1.1"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Bill Hu&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>Home</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>Archives</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>Categories</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>Tags</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>About</span></a></li><li class="nav-item"><a class="nav-link" href="https://www.billhu.xyz" target="_self"><i class="iconfont icon-code"></i> <span>Portfolio</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-01-07 19:17" pubdate>January 7, 2025 pm</time></span></div><div class="mt-1"></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</h1><div class="markdown-body"><h1 id="lec-2-a-modern-multi-core-processor"><a class="markdownIt-Anchor" href="#lec-2-a-modern-multi-core-processor"></a> Lec 2. A modern multi-core processor</h1><p>4 key concepts: 两个与parallel execution有关, 两个与challenges of accessing memory 有关</p><h2 id="parallel-execution"><a class="markdownIt-Anchor" href="#parallel-execution"></a> Parallel execution</h2><ul><li><strong>Superscalar processor</strong>: Instruction level parallelism (ILP)<ul><li>ILP读未来的指令（每个周期读两条指令），有两个fetch/decode单元和两个exec单元，能够同时执行两条指令</li></ul></li><li><strong>Multi-core</strong>: 多个processing cores<ul><li>多核之前，处理器提升重点在更大缓存，更多分支预测predictor等；同时更多晶体管（才能放得下更多缓存和更多predictor和乱序执行逻辑）促生更小的晶体管，促进更高的计算机主频</li><li>2004年多核出现之后，人们在一个chip上放多个processor，用更多晶体管放更多核心。</li></ul></li><li><strong>SIMD processing</strong> (aka <strong>Vector processing</strong>): 多个ALU(同一个core内)<ul><li>仍然只需要一个fetch/decode单元，多个ALU。</li><li>conditional execution: 如果想simd的程序块有if else，要通过mask处理</li><li>手写avx代码（cpu指令）是<strong>explicit SIMD</strong>; 而GPU是<strong>implicit SIMD</strong>，因为compiler生成的并不是并行指令（是普通的scalar instructions），只有GPU硬件运行才是SIMD的</li></ul></li></ul><h2 id="accessing-memory"><a class="markdownIt-Anchor" href="#accessing-memory"></a> Accessing memory</h2><ul><li><p><strong>cache</strong>: <strong>reduce latency</strong></p></li><li><p><strong>prefetching</strong> reduces stalls: <strong>hides latency</strong></p></li><li><p><strong>Multi-threading</strong>, <strong>interleave</strong> processing of multiple threads</p><ul><li>跟prefetching一样，也是hide latency，不能reduce latency</li><li>指的是：开多个线程，在一个线程卡住的时候执行别的线程</li><li>在下图中，创建thread1的时候不仅仅创建thread1，还会告诉电脑创建了thread 2 3 4，硬件检测线程是否发生了stall（被等待内存操作卡住），如果发生了stall会很快切换到别的线程，想juggling一样。硬件决定如何juggle这些线程。</li><li>这样memory latency仍然存在，但是被hide了。memory latency在后台发生，前台CPU一直在执行有用的工作。</li><li>这种操作会导致单个线程的执行时间变长（因为thread1从runnable到重新开始执行有一段空挡（这段空隙在执行thread 2 3 4）。</li><li>需要更多硬件资源，存储线程的register等状态信息，这样切换线程才会快。且需要较大的memory bandwidth。</li></ul></li></ul><img src="/2025/054_cmu_15618/interleave-thread.webp" srcset="/img/loading.gif" lazyload><p>GPU设计成处理大量数据（远大于核内缓存的数据量）。</p><p>与CPU内存相比，GPU显存带宽更高，但延迟更高。</p><h2 id="关于nv-gpu架构的一些备注"><a class="markdownIt-Anchor" href="#关于nv-gpu架构的一些备注"></a> 关于NV GPU架构的一些备注</h2><p>一个GPU有多个SM；每个SM上有一块shared memory；同时每个线程有自己的registers和local memory，但如果线程registers超出，则会溢出到global memory。</p><p>因为一个block保证在一个SM上，所以threads within a block可以访问__shared__ memory。</p><p>Warp是硬件规定好的，通常为32-threads，warp内一定所有线程在任意时刻都做同一件事情（如果遇到branch divergence，则会被mask out，但warp仍然同一时刻执行同一个branch）</p><p>假设block size是256（程序员指定的），warp为32（硬件规定），那么threads in the same block可能在不同的warp里，threads within a block可能同一时刻在执行不同的指令，因为它们在不同的warp调度不同。</p><h1 id="lec-3-progpramming-models"><a class="markdownIt-Anchor" href="#lec-3-progpramming-models"></a> Lec 3. Progpramming Models</h1><h2 id="abstraction-vs-implementation"><a class="markdownIt-Anchor" href="#abstraction-vs-implementation"></a> Abstraction vs Implementation</h2><p>abstraction和implementation不是一个东西！</p><p><strong>ISPC</strong> (Intel SPMD Program Compiler)</p><p>SPMD: single program multiple data</p><p>一个ISPC计算sin(x)的例子</p><ul><li><p>Interleaved</p><ul><li><img src="/2025/054_cmu_15618/ispc-sinx-interleave.webp" srcset="/img/loading.gif" lazyload></li><li><pre class="highlight"><code class="c">export <span class="hljs-type">void</span> <span class="hljs-title function_">sinx</span><span class="hljs-params">(
  uniform <span class="hljs-type">int</span> N, uniform <span class="hljs-type">int</span> terms,
  uniform <span class="hljs-type">float</span>* x, uniform <span class="hljs-type">float</span>* result)</span> &#123;
  <span class="hljs-comment">// assume N % programCount = 0</span>
  <span class="hljs-keyword">for</span> (uniform <span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;N; i+=programCount) &#123;
    <span class="hljs-type">int</span> idx = i + programIndex;
    <span class="hljs-comment">// 不重要</span>
  &#125;
&#125;
&lt;!--code￼<span class="hljs-number">0</span>--&gt;

</code></pre></li></ul></li></ul><p>在这个示例中blocked比interleaved更好。</p><p>因为每个iteration工作量完全相同，SIMD指令load连续内存（直接_mm_load_ps1）比load不连续内存(这种操作叫gather，只在AVX2及以后才支持）更快。</p><p>我们可以使用<code>foreach</code>来代替。</p><p><code>foreach</code>表示循环中的每一次iteration都是独立的，由ISPC决定如何分配</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">foreach(i = <span class="hljs-number">0</span> ... N) &#123;<br>	<span class="hljs-comment">// index ...</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>ISPC: abstraction VS. Implementation<ul><li>Programming model: SPMD<ul><li>程序员想的是，我们有programCount个逻辑指令流，写代码也按照这样的abstration去写</li></ul></li><li>Implementation: SIMD<ul><li>ISPC输出SSE4或AVX这种指令来实现逻辑操作</li></ul></li></ul></li></ul><h2 id="four-models-of-communication"><a class="markdownIt-Anchor" href="#four-models-of-communication"></a> Four Models of Communication</h2><ul><li><p><strong>Shared Address Space</strong> model</p><ul><li><p>共享内存，不同线程通过读写同一块内存来通信</p></li><li><p>很多人认为这是最方便易用的model，因为这和sequential programming最相近</p></li><li><p>每个处理器都能访问任意内存地址</p></li><li><p><strong>Uniform memory access</strong> time; Symmetric shared-memory multi-processor (SMP), : 指每个处理器都能访问内存且访问内存所需时间一致。<strong>Scalability不好。</strong></p><img src="/2025/054_cmu_15618/hw-shared-address.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>Non-uniform memory access (NUMA)</strong>: 每个处理器都能访问完整内存，但所需时间不一致。这样<strong>Scalability</strong>比较好。但是performance tuning可能需要更多精力。<img src="/2025/054_cmu_15618/NUMA.webp" srcset="/img/loading.gif" lazyload></p></li></ul></li><li><p><strong>Message Passing</strong> model</p><ul><li>线程之间不共享内存（address space不共享），只能通过发送和接收messages通信</li><li>相比shared memory的优点：不需要别的硬件，可以纯软件做，实现起来简单。常用的库是<strong>MPI</strong> (message passing interface)</li><li>现代很常用的操作是，在一个节点（节点内是多核的）内用shared address，在不同节点间用message passing</li></ul></li></ul><div class="note note-info"><p>这里很搞：abstraction can target different types of machines.</p><p>分清<strong>abstraction</strong>和<strong>implementation</strong>的区别！</p><p>比如说：</p><p>message passing 这个 <strong>abstraction</strong> 可以使用硬件上的 shared address space 来 <strong>implement</strong>。</p><ul><li>发送/接收消息就是读写message library的buffer。</li></ul><p>shared address space 这个 <strong>abstraction</strong> 在不支持硬件 shared address space 的机器上也可以用软件来 <strong>implement</strong> (但是低效)</p><ul><li>所有涉及共享变量的page都标记成invalid，然后使用page-fault handler来处理网络请求</li></ul></div><ul><li><p><strong>The data-parallel</strong> model</p><ul><li>上面的shared address space和message passing更general</li><li>data-parallel更specialized, rigid。</li><li>在不同的数据（数据块）上执行相同的操作。</li><li>通常用SPMD的形式：<strong>map (function, collection)</strong>，其中对所有的数据都做function的操作，function可以是很长的一段逻辑，比如一个loop body。collection是一组数据，可以包含多个数据。</li><li><strong>gather/scatter</strong>: gather是把本来不连续的数据按照index放到一起，scatter是把本来连续的数据分散开。<img src="/2025/054_cmu_15618/gather.webp" srcset="/img/loading.gif" lazyload></li></ul></li><li><p><strong>The systolic arrays</strong> model</p><ul><li><p>读内存太慢了，memory bandwidth会成为瓶颈</p></li><li><p>所以要<strong>避免不必要的内存读取</strong>，尽量只读一次内存，做完所有需要用到这块内存的操作，再写回去。</p></li><li><p>示例：矩阵乘法 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2VrnkXd9QR8">https://www.youtube.com/watch?v=2VrnkXd9QR8</a></p></li></ul></li></ul><p>概括four models of communication</p><ul><li>shared address space: 共享内存通信</li><li>message passing: 发消息通信</li><li>data parallel: 对一大块数据分块执行相同操作</li><li>systolic arrays: 减少内存读取</li></ul><h1 id="lec-5-parallel-programming-basics"><a class="markdownIt-Anchor" href="#lec-5-parallel-programming-basics"></a> Lec 5. Parallel Programming Basics</h1><img src="/2025/054_cmu_15618/parallel-steps.webp" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Decomposition</strong></p><p>把问题分解成 <strong>tasks</strong></p><p>Main idea: 创造至少能把机器占满的tasks数量，通常对于t个processor会给多于t个task，并且要让这些task尽可能 <strong>independent</strong>.</p><p><strong>Amdahl’s Law</strong>: 程序中有S部分只能顺序运行（无法用并行加速）则整个程序的speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">\leq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7719400000000001em;vertical-align:-.13597em"></span><span class="mrel">≤</span></span></span></span> 1/S.</p><p>通常是programmer负责decomposition。</p></li><li><p><strong>Assignment</strong></p><p>Goal: <strong>balance workload, reduce communication costs</strong></p><p>can be performed <strong>statically</strong> or <strong>dynamically</strong></p><ul><li>statically: e.g. ISPC <code>foreach</code></li><li>dynamically: e.g. ISPC <code>launch tasks</code> 运行的时候会维护线程池，线程池中的线程从任务队列中读。这样做的优点是runtime workload balance.</li></ul></li><li><p><strong>Orchestration</strong></p><p>Goal: <strong>reduce communication/sync cost</strong>, preserve locality of data reference, reduce overhead</p><p>需要考虑机器的特性（上面的decomposition和assignment不用太考虑）。</p><p>包括</p><ul><li><strong>structuring communication</strong>: 信息传递模型 e.g. 传一个chunk数据而不是只传一个byte，节约overhead</li><li><strong>adding synchronization</strong> to preserve dependencies</li><li>organizing <strong>data structures</strong> in memory</li><li><strong>scheduling</strong> tasks</li></ul></li><li><p><strong>Mapping</strong> to hardware</p><p>对程序员来说是optional的。programmer可以显式制定哪个thread跑在哪个processor上。</p><ul><li>mapping by <strong>OS</strong>: e.g. pthread</li><li>mapping by <strong>compiler</strong>: e.g. ISPC maps ISPC program instances to vector instruction lanes</li><li>mapping by <strong>hardware</strong> e.g. GPU map CUDA threads to GPU cores</li></ul><p>Mapping 还能有不同的decisions，比如说</p><ul><li>Place <strong>related</strong> threads on the same processor: 最大化locality，共享数据，减少通讯成本</li><li>Place <strong>unrelated</strong> threads on the same processor: 可能一个thread受制于内存带宽，另一个受制于计算时间，这两个thread放在一起可以让处理器利用率更高</li></ul></li></ul><p>A parallel programming example: 2D-grid based solver</p><p>TODO here.</p><h1 id="lec-6-work-distribution-scheduling"><a class="markdownIt-Anchor" href="#lec-6-work-distribution-scheduling"></a> Lec 6. Work Distribution &amp; Scheduling</h1><p>key goals:</p><ul><li>balance workload</li><li>reduce communication</li><li>reduce extra work (overhead)</li></ul><h2 id="workload-balance"><a class="markdownIt-Anchor" href="#workload-balance"></a> workload balance</h2><ul><li><p><strong>Static assignment</strong></p><p>任务分配在运行之前就已经 <strong>pre-determined</strong></p><p>例如之前讲的blocked assignment和interleaved assignment.</p><p><strong>Zero runtime overhead</strong></p><p>当任务量可预测时可以使用。不一定要任务量相同，可预测不会变就行。</p></li><li><p><strong>Semi-static assignment</strong></p><p>可预测未来短期内的任务量</p><p>一边运行一边profile并调整任务分配（periodically profiles itself and re-adjusts assignment）</p></li><li><p><strong>Dynamic assignment</strong></p><p>任务量unpredictable.</p><ul><li><pre class="highlight"><code class="c"><span class="hljs-keyword">while</span> (<span class="hljs-number">1</span>) &#123;
  <span class="hljs-type">int</span> i;
  lock(counter_lock);
  i = counter++;
  unlock(counter_lock);  <span class="hljs-comment">// 或使用 atomic_incr(counter); 代替</span>
  <span class="hljs-keyword">if</span> (i &gt;= N) <span class="hljs-keyword">break</span>;
  <span class="hljs-comment">// do with index i</span>
&#125;
&lt;!--code￼<span class="hljs-number">2</span>--&gt;

</code></pre></li></ul></li></ul><h3 id="scheuling-fork-join-programs"><a class="markdownIt-Anchor" href="#scheuling-fork-join-programs"></a> Scheuling fork-join programs</h3><ul><li><p>Bad idea: <code>cilk_spawn</code> --&gt; <code>pthread_create</code>, <code>cilk_sync</code> --&gt; <code>pthread_join</code></p><p>因为创建kernel thread开销很大。</p><p>应该用线程池。</p></li><li><p>让idle thread 从别家thread的queue里steal work to do.</p><p><strong>continuation first</strong>:</p><ul><li>record child for later execution</li><li>child is made available for stealing by other threads (child stealing)</li><li>在遇到spawn的时候，自己执行spawn后面的任务，并把spawn出来的放在自己的work queue里，等待别的线程（如果别的线程有空闲）steal自己的任务。</li><li>如果没有stealing，那么（相比于去除所有spawn语句）执行顺序全都是反的</li></ul><p><strong>child first</strong>:</p><ul><li>record continuation for later execution</li><li>continuation is made available for stealing by other threads (continuation stealing)</li><li>遇到spawn的时候，只创建一个可被steal的项目。</li></ul></li><li><p>work queue可以用<strong>dequeue</strong> (double-ended queue)实现</p><p>每一个线程有自己的work queue，针对自己的work queue，在尾部添加，从尾部取出</p><p>如果要steal别的线程的work queue，从头部取出</p></li></ul><h1 id="lec-7-locality-communication-and-contention"><a class="markdownIt-Anchor" href="#lec-7-locality-communication-and-contention"></a> Lec 7. Locality, Communication, and Contention</h1><p>Lec6讲如何平均分配任务，Lec7讲如何降低communication开销.</p><ul><li><p><strong>synchronous (blocking)</strong> send and receive</p><img src="/2025/054_cmu_15618/synchronous-send-recv.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>non-blocking asynchronous</strong> send and receive</p></li></ul><p>send()和recv()函数会立即返回 在后台做事</p><img src="/2025/054_cmu_15618/async-send-recv.webp" srcset="/img/loading.gif" lazyload><h2 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h2><p>使用Pipeline: <strong>Latency 不变, Throughput 增加</strong></p><p>例子：</p><ul><li><p>Communication = Overhead(橙色) + Occupancy (蓝色) + Network delay (灰色)</p></li><li><p>最长的部分是瓶颈，决定了throughput上限</p></li></ul><img src="/2025/054_cmu_15618/pipelined-communication.webp" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Overlap</strong>: communication和其它工作同时运行的时间。</p><p>我们希望能尽可能增加overlap这样communication cost才会降低。</p><p>降低overlap的方法</p><ul><li>Example 1: <strong>Asynchronous</strong> message send/recv 异步消息</li><li>Example 2: <strong>Pipelining</strong> 发送多条消息时让这个发送过程overlap</li></ul></li></ul><h2 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h2><p>Communication包含inherent和artifactual</p><ul><li><p><strong>Inherent communication</strong>: 程序算法写好的，必须发生的通信</p><ul><li>Communication-to-computation ratio: 通信量/计算量 的比值。越低越好。</li><li><strong>arithmetic intensity</strong>: 1/communication-to-computation ratio. 越高越好。</li></ul></li><li><p><strong>Artifactual communication</strong>: 所有别的通信，因为memory hierarchy导致额外的通信，例如L1/L2/L3/内存/网络之间的通信。包括：</p><p>① 系统有<strong>minimum granularity of transfer</strong>: 即使只需要读取4byte数据，也需要复制64-byte整条cache line</p><p>② 系统有<strong>rules of operation</strong>: 例如，写入内存需要先把内存读到cache line中（write-allocate）之后踢出cache line再写入内存，导致一次写入操作需要访问两次内存</p><p>③ <strong>Poor placement</strong> of data in distributed memories: 被某个processor访问最多的数据并没有放在这个processor附近</p><p>④ Finite replication <strong>capacity</strong>: 因为cache太小放不下，会被踢掉，所以有一些数据频繁被踢出/放入cache</p><p>提高<strong>locality</strong>对降低artifactual communication很重要</p></li></ul><p>提高<strong>temporal locality</strong>的例子</p><ul><li><p>by changing grid traversal order</p><img src="/2025/054_cmu_15618/change-grid-traversal-order.webp" srcset="/img/loading.gif" lazyload></li><li><p>by fusing loops</p><img src="/2025/054_cmu_15618/fusing-loops.webp" srcset="/img/loading.gif" lazyload></li><li><p>by sharing data</p><img src="/2025/054_cmu_15618/sharing-data.webp" srcset="/img/loading.gif" lazyload></li></ul><p>提高<strong>spatial locality</strong></p><ul><li><p>false sharing 不好</p></li><li><p><strong>4D array layout</strong> (<strong>blocked data layout</strong>): Embedding a 2D array within another 2D array allows page granularities to remain within a tile, making it practical to map data to local portions of physical memory (thereby reducing cache miss latencies to main memory).</p><img src="/2025/054_cmu_15618/4d-array.webp" srcset="/img/loading.gif" lazyload></li></ul><h2 id="contention"><a class="markdownIt-Anchor" href="#contention"></a> Contention</h2><p><strong>Contention</strong>: 在短时间内很多人请求同一个resource</p><p>Example: distributed work queues (让每个线程有自己的work queue)可以降低contention</p><h2 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h2><ul><li>降低communication costs<ul><li><strong>Reduce overhead</strong>: 发更少的消息数量，更长的消息内容（合并短消息）</li><li><strong>Reduce delay</strong>：提高locality</li><li><strong>Reduce contention</strong>: 把contended resource分开，例如local copies, fine-grained locks</li><li><strong>Increase overlap</strong>: 用异步消息、pipeline等 提高communication和computation的overlap</li></ul></li></ul><h1 id="lecture-9-workload-driven-perf-evaluation"><a class="markdownIt-Anchor" href="#lecture-9-workload-driven-perf-evaluation"></a> Lecture 9. Workload-Driven Perf Evaluation</h1><ul><li>Super-linear Speedup:<ul><li>processor足够多的时候，每个processor分到的数据fits in cache</li></ul></li><li>Decreasing Speedup:<ul><li>随着processor增多，communication占比太大了</li></ul></li><li>Low speedup:<ul><li>Increasing contexts are hyperthreaded contexts (?)</li></ul></li></ul><p><strong>Resource-oriented</strong> scaling properties</p><ul><li><strong>Problem constrained</strong> scaling (PC)<ul><li>更快速解决同一个问题</li></ul></li><li><strong>Memory constrained</strong> scaling (MC)<ul><li>不爆内存的情况下运行最大能完成的任务</li></ul></li><li><strong>Time constrained</strong> scaling (TC)<ul><li>同样的时间内完成更多任务</li></ul></li></ul><h2 id="simulation"><a class="markdownIt-Anchor" href="#simulation"></a> Simulation</h2><p><strong>Execution-driven</strong> simulator</p><ul><li>模拟内存，模拟内存访问</li><li>模拟器的performance通常与模拟的细节数量成反比</li></ul><p><strong>Trace-driven</strong> simulator</p><ul><li>在real machine上运行real code得到内存访问的trace，或者用execution-driven simulator生成trace</li><li>然后在模拟器上运行trace</li></ul><h1 id="lec-10-interconnects"><a class="markdownIt-Anchor" href="#lec-10-interconnects"></a> Lec 10. Interconnects</h1><h2 id="interconnect-terminology"><a class="markdownIt-Anchor" href="#interconnect-terminology"></a> Interconnect terminology</h2><p>Terminology</p><ul><li><p><strong>Network node</strong>: 网络终端，会产生或消耗traffic，例如processor cache</p></li><li><p><strong>Network interface</strong>: 把nodes和network相连</p></li><li><p><strong>Switch/Router</strong>: 将固定数量的input links与固定数量的output links相连</p></li><li><p><strong>Link</strong>: 传输信号的线缆</p><img src="/2025/054_cmu_15618/interconnect-terminology.webp" srcset="/img/loading.gif" lazyload></li></ul><p>设计interconnection需要考虑的因素</p><ul><li><p><strong>topology</strong>: 怎么相连</p><ul><li><p>topology的属性：</p><ul><li><p><strong>routing distance</strong>: nodes之间的长度，nodes相连需要多少个links (hops)</p></li><li><p><strong>diameter</strong>: 最大routing distance</p></li><li><p><strong>average distance</strong>: 平均routing distance</p></li><li><p><strong>direct / indirect</strong> networks</p><img src="/2025/054_cmu_15618/direct-indirect-networks.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>bisection bandwidth</strong></p></li><li><p><strong>blocking vs non-blocking</strong>: 如果任何两个pairs of nodes可以同时传输，不相干扰，则为non-blocking。大部分network都是blocking的</p></li></ul></li></ul></li><li><p>routing: 消息沿什么路线传输到达目的地？可以static可以adaptive</p></li><li><p>buffering and flow control</p></li></ul><img src="/2025/054_cmu_15618/topology-1.webp" srcset="/img/loading.gif" lazyload> <img src="/2025/054_cmu_15618/topology-2.webp" srcset="/img/loading.gif" lazyload><h2 id="buffering-and-flow-control"><a class="markdownIt-Anchor" href="#buffering-and-flow-control"></a> Buffering and Flow control</h2><p>和14740的第一节课讲的很像</p><h1 id="lec-11-perf-tools"><a class="markdownIt-Anchor" href="#lec-11-perf-tools"></a> Lec 11. Perf Tools</h1><h2 id="性能测试工具"><a class="markdownIt-Anchor" href="#性能测试工具"></a> 性能测试工具</h2><p><strong>GProf</strong></p><ul><li>compiler flag <code>-pg</code></li><li>places a call into every function --&gt; <strong>call graph</strong> (total time in each function)</li><li>先跑程序，然后单独使用 <code>gprof</code> 命令(不传参数)</li></ul><p><strong>Perf</strong></p><ul><li>有硬件指令测量性能计数器：cache misses, branch mispredicts, IPC, …</li><li><code>perf stat</code> (同时只能开启4个counter)</li></ul><p>VTune</p><ul><li>similar to perf: analysis across counters</li><li>有图形界面和解析</li></ul><h2 id="debug工具"><a class="markdownIt-Anchor" href="#debug工具"></a> Debug工具</h2><p>Valgrind</p><ul><li>heavy-weight, 需要 shadowing</li><li>有大量的overhead，不要用它测试performance</li><li><code>valgrind --tool=memcheck</code></li></ul><p>Address Sanitizer</p><ul><li>GCC and LLVM support, 有编译器支持</li><li>overhead比valgrind小一些</li><li><code>-fsanitize=address</code></li></ul><h2 id="advanced-analysis"><a class="markdownIt-Anchor" href="#advanced-analysis"></a> Advanced analysis</h2><p>Pin (Pintool)</p><ul><li>acts as a virtual machine: reassembles instructions</li><li>can record every single instruction/block(无跳转)/trace(可能跨函数)</li></ul><p>Contech</p><ul><li>compiler-based (uses clang+LLVM)</li><li>record control flow, mem access, concurrency</li><li>traces analyzed AFTER collection</li></ul><h2 id="summary-questions"><a class="markdownIt-Anchor" href="#summary-questions"></a> Summary questions</h2><ul><li>Reproducible?<ul><li>Do you have a workload? Is the system stable? (Stable是说每次运行性能差距不能太大)</li></ul></li><li>Workload at full CPU?<ul><li>Other users using CPU? Does workload rely heavily on IO?</li><li>使用<code>time</code> / <code>top</code> 看cpu占用时长</li></ul></li><li>Is CPU time confined to a small number of functions?<ul><li>占用时长最长的函数？算法复杂度？</li><li><code>gprof</code> / <code>perf</code></li></ul></li><li>Is there a small quantity of hot functions?<ul><li><code>perf</code> / <code>VTune</code></li></ul></li></ul><h1 id="lec-12-snooping-based-cache-coherence"><a class="markdownIt-Anchor" href="#lec-12-snooping-based-cache-coherence"></a> Lec 12. Snooping-based Cache Coherence</h1><div class="note note-info"><p>Recap:</p><ul><li><strong>write-allocate</strong>: 如果写入的内存不在cache中，则需要先把memory读到cache中再写入cache<ul><li>write-allocate与write-through和write-back都可以配合使用，但<strong>通常write-allocate与write-back搭配</strong></li></ul></li><li><strong>no-write-allocate</strong>: 直接写内存</li><li><strong>write-through</strong>: 同时写入cache和memory</li><li><strong>write-back</strong>: 只写入cache，之后再flush to memory</li></ul></div><h2 id="memory-coherence"><a class="markdownIt-Anchor" href="#memory-coherence"></a> Memory coherence</h2><p>A memory system is <strong>coherent</strong> if:</p><ul><li><p>the results of a parallel program’s execution are such that for each memory location, there is a hypothetical <strong>serial order of all program operations</strong> (executed by all processors) to the location that is consistent with the results of execution.</p><p>与某一个serial的内存访问顺序的结果一致</p></li></ul><p>Said differently</p><p><strong>Definition</strong>: A memory system is <strong>coherent</strong> if</p><ul><li><strong>obeys program order</strong>: 一个processor先write再read一定读到新值</li><li><strong>write-propagation</strong>: P1先write，一段时间后(suffciently separated in time) P2再read，则P2一定读到新值。注意此处需要相隔多久并没有定义。</li><li><strong>write serialization</strong>: 两个processor写入同一个位置，则大家都必须agree on one order, 大家都同意这个顺序</li></ul><p>可以用软件或硬件的方法解决</p><p>软件解法：OS采用page fault来propagate writes</p><p>硬件解法：Snooping based (本节课), Directory-based (下节课)</p><h2 id="snooping"><a class="markdownIt-Anchor" href="#snooping"></a> Snooping</h2><p>Cache controllers monitor (<strong>snoop</strong>) memory operations.</p><p>任意一个processor修改cache都会broadcast通知其它所有人</p><p>现在cache controller不仅需要响应处理器，还需要响应其它cache的broadcast</p><p>Assume write-through:</p><img src="/2025/054_cmu_15618/write-through-invalidation.webp" srcset="/img/loading.gif" lazyload><p>Write-through is inefficient: 因为每次write操作都需要写入内存，也是因为此原因write-through不常见</p><h2 id="msi-write-back-invalidation-protocol"><a class="markdownIt-Anchor" href="#msi-write-back-invalidation-protocol"></a> MSI write-back invalidation protocol</h2><p>Cache line加上dirty bit，如果dirty bit=1，代表处理器拥有这条cache line的exclusive ownership (<strong>Dirty = Exclusive</strong>)</p><p>如果其它processor想读同一条cache line，能从具有exclusive ownership的processor的cache中读（owner is responsible for supplying the line to other processors)</p><p>Processor也只能写入M-state的cache line; Processor能随时修改M-state的cache line不用通知他人</p><p>Cache controller监听是否有别人想要exclusive access，如果有，则自己必须要invalidate</p><img src="/2025/054_cmu_15618/msi.webp" srcset="/img/loading.gif" lazyload><h2 id="mesi-invalidation-protocol"><a class="markdownIt-Anchor" href="#mesi-invalidation-protocol"></a> MESI invalidation protocol</h2><p>在MSI中read会有两种情况</p><ul><li>BusRd: 从I转成S，即使并没有真正shared</li><li>BusRdX: 从S转成M</li></ul><p>添加一个新的<strong>E-state</strong> (<strong>exclusive clean</strong>) 代表exclusive但尚未修改</p><p>读取时询问别的cache有没有这条cache line，如果别人也有则从I进S，否则从I进E</p><p>从E升级到M不需要通知他人</p><img src="/2025/054_cmu_15618/MESI.webp" srcset="/img/loading.gif" lazyload><h2 id="other-mesif-moesi"><a class="markdownIt-Anchor" href="#other-mesif-moesi"></a> Other (MESIF, MOESI)</h2><ul><li><strong>MESIF</strong>: F = Forward<ul><li>类似与MESI，但是在多个cache都shared的时候，有一个cache不在S而在F</li><li>F holded my most recent requester</li><li>F负责service miss（给别人提供数据），作为对比，MESI中所有S都会做出响应</li><li>I不能直接进入S，你要么进E(如果没人有), 要么进F(别人也有cache，但你是most recent requester所以你要负责给别的cache line提供data)。E和F随后有可能进入S。</li><li>Intel处理器用的是MESIF</li></ul></li><li><strong>MOESI</strong>: O = Owned but Not Exclusive<ul><li>在MESI中，从M转成S需要flush to memory，MOESI添加O-state，从M转成O但不flush内存</li><li>在O时，这条cache line负责给别人提供data，但不flush进内存</li></ul></li></ul><img src="/2025/054_cmu_15618/MOESIF.webp" srcset="/img/loading.gif" lazyload><h2 id="inclusive-property"><a class="markdownIt-Anchor" href="#inclusive-property"></a> Inclusive property</h2><p>L1 L2 L3多级缓存</p><p>如果让所有L1缓存间和L2缓存间都interconnecting，那么效率低，所以让L2之间interconnect，并让L2 inclusive，即L2缓存中包含所有L1缓存，由L2控制L1的invalidate等</p><p><strong>Inclusive property</strong> of caches: all lines in closer (to processor) cache are also in farther (from processor) cache. e.g. contents of L1 are a subset of L2</p><p>如果单纯让L2比L1大，不能自动保证inclusion</p><p>需要让L1和L2相互交流，L2中维护一个“是否在L1中”的bit</p><img src="/2025/054_cmu_15618/multilevel-cache-inclusion.webp" srcset="/img/loading.gif" lazyload><h2 id="gpu-dont-have-cache-coherence"><a class="markdownIt-Anchor" href="#gpu-dont-have-cache-coherence"></a> GPU don’t have cache coherence</h2><p>每个cache都必须监听并对所有broadcast做出反应，这样interconnect开销会随着processor数量增长而增长</p><p>所以Nvidia GPU没有cache coherence，而是用atomic memory operation绕过L1 cache访问global memory</p><img src="/2025/054_cmu_15618/nvgpu-no-cache-coherence.webp" srcset="/img/loading.gif" lazyload><h1 id="lec-13-directory-based-cache-coherence"><a class="markdownIt-Anchor" href="#lec-13-directory-based-cache-coherence"></a> Lec 13. Directory-based Cache coherence</h1><p>上一节课讲Snooping</p><p>Snooping-based cache coherence需要依赖broadcast工作，每次cache miss时都要与其它所有cache通信</p><p>存在scalability问题</p><p>One possible solution: <strong>hierarcical snooping</strong>。缺点是root会成为瓶颈；延迟；不能用于不同的拓扑结构</p><h2 id="scalable-cache-coherence-using-directories"><a class="markdownIt-Anchor" href="#scalable-cache-coherence-using-directories"></a> Scalable cache coherence using <strong>Directories</strong></h2><p>在一个地方存directory，每条cache line的directory信息存储着所有cache中这条cache line的状态</p><p>用point-to-point messages代替broadcast来传数据</p><p>Directory中包含 <strong>dirty bit</strong> 和 <strong>presence bit</strong>, 第k个presence bit代表第k个processor是否有这条cache line</p><p>Distributed directory: directory与memory大小同步增长</p><img src="/2025/054_cmu_15618/simple-directories.webp" srcset="/img/loading.gif" lazyload><p>Example 1: <strong>Read miss to clean line</strong></p><p>Processor 0把位于1的内存数据读到了自己的local cache 里，对应的directory记录P0有值</p><img src="/2025/054_cmu_15618/directory-eg1.webp" srcset="/img/loading.gif" lazyload><p>Example 2: <strong>Read miss to dirty line</strong></p><p>本来P2的local cache中有dirty的数据</p><p>P0想读，发送read miss消息，P1告诉P0目前P2有dirty数据，P0收到后去向P2请求数据，P2将数据发给P0并将状态设置为shared，位置1的directory presence bit记录目前P0和P2有数据</p><img src="/2025/054_cmu_15618/directory-eg2.webp" srcset="/img/loading.gif" lazyload><p>Example 3: <strong>Write miss</strong></p><p>P0有一条cache line，将要写入这条cache line，因此先请求找出有哪些Processor目前有这条cache line（找出sharer ids）然后向它们（P1和P2）发送invalidate请求，收到P1和P2的ack之后代表它们两个已经invalidate，此时再进行写入内存操作。</p><img src="/2025/054_cmu_15618/directory-eg3.webp" srcset="/img/loading.gif" lazyload><p><strong>Advantage of directories</strong>:</p><ul><li>在read时，directory能直接告诉节点应该去问谁要数据，仅需要点对点通信：如果line is clean, 从home node要；如果line is dirty, 从owner node要。</li><li>在write时，directory告诉sharer id，工作量取决于有多少节点在共享数据。极端情况，如果所有cache都在共享数据，则需要与所有节点通信，像broadcast一样。</li></ul><h2 id="limited-pointer-schemes"><a class="markdownIt-Anchor" href="#limited-pointer-schemes"></a> Limited pointer schemes</h2><p>presense bit需要占用存储空间，会导致storage overhead</p><p>Reducing storage overhead</p><ul><li>increase cache line size: 让占比减小（M减小）</li><li>group multiple processors into a single directory node (让P减小)</li><li>除此之外还能使用 <strong>limited pointer scheme</strong> (降低P) 和 <strong>sparse directories</strong></li></ul><p><strong>Limited pointer schemes</strong>: 只存指针（指针=processor的id）</p><p>如果指针溢出，有几种不同的实际方法</p><ul><li>指针溢出时改为broadcast（添加一个additional bit代表指针不够用）</li><li>设置最大共享者数量，不允许超出，如果超出，老的sharer被移除</li><li>指针溢出时改为bit vector representation</li></ul><h2 id="sparse-directories"><a class="markdownIt-Anchor" href="#sparse-directories"></a> Sparse directories</h2><p>Key observation: majority of memory is NOT resident in cache.</p><p>Sparse directories只存一个指针，而在processor的cache line上存prev和next指针</p><img src="/2025/054_cmu_15618/sparse-directories.webp" srcset="/img/loading.gif" lazyload><p>优化：<strong>Intervention forwarding</strong>, <strong>Request forwarding</strong></p><hr><h1 id="lec-16-memory-consistency"><a class="markdownIt-Anchor" href="#lec-16-memory-consistency"></a> Lec 16. Memory Consistency</h1><p>Correct behaviour for a parallel memory hierarchy: <strong>reading should return the lastest value written (by any thread)</strong> （write之后read一定要读出最新的结果）</p><ul><li><p>注：因为write的结果只有在read发生时才能看见，所以我们只关注write之后的read</p></li><li><p>问题: <strong>latest</strong> 的定义是什么？单线程很好定义，但多线程不一定</p><ul><li>e.g. 如果处理器之间通信需要10个周期，那么p1读2时钟周期前p0写入的值，如何获知p0改过值？类似于，你看见的星星是100秒前的</li></ul></li><li><p>因此重新给出一个定义：</p></li></ul><div class="note note-success"><p><strong>writes from any particular thread must be consistent with program order</strong> 对于一个线程的所有write，顺序必须相同</p><p><strong>across threads</strong>: writes must be consistent with a <strong>valid interleaving of threads</strong></p></div><ul><li><p>(是hypothetical的interleaving，并非真实存在的interleaving)</p></li><li><p>像是有一个指针，同时只能指向一个处理器</p><img src="/2025/054_cmu_15618/mem-consistency-interleave.webp" srcset="/img/loading.gif" lazyload></li></ul><p>为什么memory consistency复杂？</p><ul><li>处理器为了hide memory latency会对指令重排序，单线程没问题，多线程会导致错误</li><li>write buffer在多核下会导致错误</li></ul><p>TODO：</p><p>关键词: twist (气球间添加twist)</p><p>MFENCE</p><h1 id="lec-17-heterogeneous-parallel-hardware-specialization"><a class="markdownIt-Anchor" href="#lec-17-heterogeneous-parallel-hardware-specialization"></a> Lec 17. Heterogeneous Parallel &amp; Hardware Specialization</h1><p>Recap: <strong>Amdahl’s law</strong></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>f</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><mi>f</mi><mi>n</mi></mfrac></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathrm{speedup}(f,n) = \frac{1}{(1-f) + \frac{f}{n}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">s</span><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">e</span><span class="mord mathrm">d</span><span class="mord mathrm">u</span><span class="mord mathrm">p</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.4886559999999998em;vertical-align:-1.1672159999999998em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.177784em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.9322159999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.446108em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1672159999999998em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><ul><li><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span>: 程序中可parallelizable部分的占比</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span>: 处理器数量</li></ul></li></ul><p>Rewrite <strong>Aldahl’s Law in terms of resource limits</strong></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>f</mi><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><mrow><mo fence="true">[</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>f</mi></mrow><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>+</mo><mfrac><mi>f</mi><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><mi>n</mi><mi>r</mi></mfrac></mrow></mfrac><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{speedup}(f,n,r) = 1 / \left[\frac{1-f}{\mathrm{perf}(r)} + \frac{f}{\mathrm{perf}(r) \cdot \frac{n}{r}}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">s</span><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">e</span><span class="mord mathrm">d</span><span class="mord mathrm">u</span><span class="mord mathrm">p</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.481em;vertical-align:-1.0310000000000001em"></span><span class="mord">1</span><span class="mord">/</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.695392em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0310000000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span>: 程序中可parallelizable部分的占比</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span>: 总共的处理器资源</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span>: 每个处理器核心所能分配到的资源<ul><li>假设每个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi mathvariant="normal">/</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">n/r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span> 核心顺序执行的性能为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{perf}(r)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span></span></li></ul></li></ul><img src="/2025/054_cmu_15618/heterogeneity-r-example.webp" srcset="/img/loading.gif" lazyload><ul><li>Example: 上面这两张图total processing resources n都等于16，左图r=4，右图r=1</li></ul><img src="/2025/054_cmu_15618/heterogeneity-example-1.webp" srcset="/img/loading.gif" lazyload><p>上图的例子</p><ul><li>横坐标为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span>, 代表每个核心占多大面积。总面积<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span>固定 (total chip resources keeps same)，越往左代表每个核心越小，核心数量越多 (many small cores)，越往右代表每个核心越大，核心数量越少 (fewer fatter cores)</li><li>不同线对应不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> 值（程序可并行部分的占比）</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><msqrt><mo stretchy="false">(</mo></msqrt><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{perf}(r) = \sqrt(r)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.24em;vertical-align:-.30499999999999994em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.935em"><span class="svg-align" style="top:-3.2em"><span class="pstrut" style="height:3.2em"></span><span class="mopen" style="padding-left:1em">(</span></span><span style="top:-2.8950000000000005em"><span class="pstrut" style="height:3.2em"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em"><svg width="400em" height="1.28em" viewBox="0 0 400000 1296" preserveAspectRatio="xMinYMin slice"><path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.30499999999999994em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span></span></li><li>纵坐标是相比于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 的单核处理器的speedup</li><li>课堂quiz题：为什么上面的图都converge到一点？因为r足够大的时候处理器只有一颗核心，所以不管f是多少执行结果都全部相同</li></ul><img src="/2025/054_cmu_15618/asymmetric-cores.webp" srcset="/img/loading.gif" lazyload><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>f</mi><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>r</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><mrow><mo fence="true">[</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>f</mi></mrow><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>+</mo><mfrac><mi>f</mi><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{speedup}(f,n,r) = 1 / \left[ \frac{1-f}{\mathrm{perf}(r)} + \frac{f}{\mathrm{perf}(r) + (n-r)} \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">s</span><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">e</span><span class="mord mathrm">d</span><span class="mord mathrm">u</span><span class="mord mathrm">p</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-.95003em"></span><span class="mord">1</span><span class="mord">/</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0"><span class="delimsizing size3">]</span></span></span></span></span></span></span></p><p>上面的例子：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">6</span></span></span></span>, 有一颗<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r=4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span>的大核，剩下12颗<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>的小核</p><p>分母<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>n</mi><mo>−</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{perf}(r) + (n-r)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span></span>的意思是：一颗<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{perf}(r)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mclose">)</span></span></span></span>的处理器 + <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">n-r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span>颗<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathrm{perf}(1)=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathrm">p</span><span class="mord mathrm">e</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:.07778em">f</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>的处理器</p><img src="/2025/054_cmu_15618/asymmetric-speedup.webp" srcset="/img/loading.gif" lazyload><h2 id="heterogeneous-processing-异构处理器"><a class="markdownIt-Anchor" href="#heterogeneous-processing-异构处理器"></a> Heterogeneous processing 异构处理器</h2><ul><li><p>Observation: real world applications 的 workload characteristics 特性不同</p><ul><li>例如：有的部分能并行，有的不行；有的部分能使用SIMD，有的不行，因为divergent control flow；有的部分具有predictable data access，有的不行</li></ul></li><li><p>Idea: 最高效的处理器是 heterogeneous mixture of resources</p></li><li><p><strong>Energy-contrained computing</strong></p></li><li><p>Tradeoff: efficiency and <strong>programmability</strong></p><ul><li>debug更难，例如在GPU上debug就比在CPU上难</li><li>FPGA: efficient, 但难以program</li><li>ASIC (例如硬件video解码器): 更加efficient，但不能program，且需要大量资金来design/verify/create</li></ul></li></ul><h2 id="challenges-of-heterogeneous-designs"><a class="markdownIt-Anchor" href="#challenges-of-heterogeneous-designs"></a> Challenges of heterogeneous designs</h2><ul><li><p>到目前为止，目标是: keep all processors busy all the time</p></li><li><p>Heterogeneous的目标是：<strong>use preferred processor for each task</strong></p></li></ul><h1 id="lec-18-domain-specific-programming"><a class="markdownIt-Anchor" href="#lec-18-domain-specific-programming"></a> Lec 18. Domain-specific Programming</h1><p>不可能三角: 高性能、通用性（完整性）、方便编程，三个里面只能选两个</p><p><strong>Domain-specific languages (DSL)</strong> 选择了high performance和productivity，代价是失去了generality/completeness (不能作为general purpose)</p><img src="/2025/054_cmu_15618/dsl-triangle.webp" srcset="/img/loading.gif" lazyload><h2 id="example-1-lizst"><a class="markdownIt-Anchor" href="#example-1-lizst"></a> Example 1: Lizst</h2><p>对mesh做操作的一种编程语言</p><p>程序员不负责implement the mesh.</p><p>程序员指定mesh的类型(regular, irregular)和topology拓扑结构，编译器根据程序员想对mesh做的操作来选择如何represent the mesh.</p><p>为了生成能parallelizable的代码，lizst编程语言和编译器做了这些事</p><ul><li>identify <strong>parallelism</strong>: 做dependency analysis, 没有dependency的代码可以同时执行</li><li>identify <strong>data locality</strong>: 如果有dependency则bad for parallelism，但是good for locality</li><li>reason about required <strong>synchronization</strong></li></ul><h2 id="example-2-halide"><a class="markdownIt-Anchor" href="#example-2-halide"></a> Example 2: Halide</h2><p>a DSL for image processing</p><p>快速生成vectorize (SIMD)和parallel的代码</p><h1 id="lec-19-dsl-on-graphs"><a class="markdownIt-Anchor" href="#lec-19-dsl-on-graphs"></a> Lec 19. DSL on graphs</h1><p>设计图计算的系统programming system需要考虑什么</p><ul><li>what are the <strong>fundamental operations</strong> we want to be easy to express and efficient to execute</li><li>what are the <strong>key optimizations</strong> performed by the best implementations of these operations</li></ul><p>Graph computation的例子: <strong>Page Rank</strong></p><ul><li><p>经典的iterative graph algorithm, 用于计算网页重要性，节点为网页，边为网页链接</p></li><li><p><strong>GraphLab</strong>: 用于描述iterative computations on graphs的系统。提供C++ runtime</p><ul><li><p>GAS (Gather-Apply-Scatter) 模型</p><p>Gather 收集: 从邻居节点收集信息</p><p>Apply 应用: 更新当前节点的值</p><p>Scatter 散布: 将新的值更新至邻居节点或边上</p></li><li><p>GraphLab的调度机制</p><ul><li>Synchronous: 每轮更新全部节点</li><li>Dynamic: 节点动态发出信号signal，根据需求进行更新</li></ul></li><li><p>GraphLab提供不同的consistency models 一致性模型</p><ul><li>Full Consistency</li><li>Edge Consistency</li><li>Vertex Consistency</li></ul></li><li><p>性能优化：瓶颈在于访存而非计算(aritimetic intensity低), 因此主要策略有</p><ul><li>Graph Reorganization 图结构重组：提高locality</li><li>Graph Compression 图压缩</li></ul></li></ul></li></ul><h1 id="lec-20-part1-implement-message-passing"><a class="markdownIt-Anchor" href="#lec-20-part1-implement-message-passing"></a> Lec 20 Part1. Implement Message Passing</h1><p>Threads拥有<strong>private address spaces</strong>, 只能够通过send/receive messages来通信</p><p>硬件不需要实现system-wide loads and stores.</p><p>能够通过将commodity systems连接起来形成巨大的parallel machine (message passing is a programming model for <strong>clusters</strong>)</p><p>Message Passing Implementation Options</p><ul><li><p><strong>synchronous</strong></p><ul><li>send操作在匹配的接收操作完成后才结束</li><li>receive操作在所有数据传输完成之后才结束</li><li>接收方没有contention，不需要buffering</li></ul></li><li><p><strong>asychronous</strong></p><ul><li><p>send completes <strong>after send buffer may be reused</strong></p></li><li><p>分为optimistic和conservative</p><ul><li><p><strong>optimistic</strong>: 发送端不需要等待接收端响应即可完成发送操作，但可能需要额外的缓冲空间</p><p>接收方接收时，如果失败，则allocate data buffer</p><p>好处：发送方不会因为等待接收方准备而stall</p><p>坏处：可能需要message layer有额外的存储空间</p></li><li><p><strong>conservative</strong>: 需要接收方先准备好，发送方才会发送数据，避免缓冲区溢出问题</p><p>发送方先发送send-ready request (assume fail), 接收方回复receive-ready request, 然后再开始真正的传数据</p></li></ul></li><li><p>Optimistic对短消息友好(低延迟), 但Conservative整体上更安全(不会出现buffer overflow)</p></li></ul></li><li><p><strong>Hybrid</strong> approach: <strong>Credit-Based Async Message Passing</strong></p><p>发送方预先分配有限的缓冲空间(credit)，只有确认有足够的credit时才会optimistically发送数据，否则采取conservative的方式</p><p>Tracking credit limit的方式：</p><p>发送方拥有一定数量的credit，每发送一个消息都会消耗一定的信用</p><p>当接收方处理完消息、释放出缓冲区空间（增加了可用credit）后，不会单独向发送方发送专门的信用更新消息，而是将这些信用更新信息piggyback在返回给发送方的其他消息之中（减少发送消息的数量）</p></li><li><p>Challenge: avoiding <strong>fetch deadlock</strong></p><ul><li><p><strong>deadlock</strong>: 每个节点都需要持续接收消息，即使自己已经不能再发送新的消息。</p><p>如果接收到的transaction消息是一个request，那么需要发送response，但因为buffer满了无法发送出去，就会导致fetch deadlock请求死锁</p></li><li><p>Approaches:</p><ul><li><p>Logically independent request/reply networks</p><p>将请求和响应信息使用不同的通道或网络分别传输</p><p>即使reply通道堵塞，request通道也不受影响，可以继续接收请求</p><p>实现方法可通过physical networks(物理独立网络)和virtual channels(同一个物理网络上划分多个逻辑队列)</p></li><li><p>Bound requests and reserve input buffer space</p><p>主动限制每个节点能同时接收的请求数目</p><p>假如共有P个节点，每个节点最多接收K个请求，则最多需要为每个节点预留 K(P-1)个request的buffer，还需要额外预留K个response的buffer</p></li><li><p>NACK on input buffer full</p><p>节点buffer overflow不能接收新消息，会告诉发送方NACK (negative acknowledge)告知自己满了</p></li></ul></li></ul></li></ul><h1 id="lec-20-part-2-implement-parallel-runtimes"><a class="markdownIt-Anchor" href="#lec-20-part-2-implement-parallel-runtimes"></a> Lec 20 Part 2. Implement Parallel Runtimes</h1><p>pthread, OpenMP, cilk</p><h1 id="lec-21-synchronization"><a class="markdownIt-Anchor" href="#lec-21-synchronization"></a> Lec 21. Synchronization</h1><h2 id="锁的实现"><a class="markdownIt-Anchor" href="#锁的实现"></a> 锁的实现</h2><ul><li><p><strong>busy waiting / spinning</strong>: 循环等待。如果scheduling overhead大于预计等待时间则spin更好</p></li><li><p><strong>blocking</strong>: 如果卡住了就block，让OS de-schedule这个thread</p></li><li><p><strong>test-and-set</strong> lock:</p><p>lock: <code>ts R0, mem; bnz R0, lock</code>; unlock: <code>st mem, 0</code></p><p>问题：拿锁失败的过程需要读取内存，造成bus contention (high interconnect traffic, poor scaling)</p></li><li><p><strong>test and test-and-set</strong> lock:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span> <span class="hljs-title function_">Lock</span><span class="hljs-params">(<span class="hljs-type">int</span> *lock)</span> &#123;<br>	<span class="hljs-keyword">while</span> (<span class="hljs-number">1</span>) &#123;<br>		<span class="hljs-keyword">while</span> (*lock != <span class="hljs-number">0</span>);  <span class="hljs-comment">// 当别人占用锁时等待</span><br>		<span class="hljs-keyword">if</span> (test_and_set(*lock) == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span>;  <span class="hljs-comment">// 当锁被释放时完成lock</span><br>	&#125;<br>&#125;<br><span class="hljs-type">void</span> <span class="hljs-title function_">Unlock</span><span class="hljs-params">(<span class="hljs-keyword">volatile</span> <span class="hljs-type">int</span> *lock)</span> &#123;<br>  *lock = <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在uncontended时会有略高的latency，因为需要test然后再test-and-set，但是由于中间尝试拿锁只需要读local cache，所以会有显著更少的interconnect traffic</p></li><li><p><strong>test-and-set with back off</strong></p><p>contention时有更高的latency</p><p>traffic更少，scalability因此也会更好，但是会有严重的不公平（新来的request back off时间短）</p></li><li><p><strong>ticket lock</strong>: 解决锁释放时大家一拥而上的问题，lock时++next_ticket, 当now_serving轮到自己时才会拿锁</p></li><li><p><strong>array-based lock</strong>: 让每个processor在不同的内存地址spin</p></li><li><p><strong>queue-based lock (MCS lock)</strong>: 创建waiter的队列（链表）建立global lock和my lock (local)</p></li></ul><h2 id="barrier的实现"><a class="markdownIt-Anchor" href="#barrier的实现"></a> Barrier的实现</h2><p>普通的shared counter barrier存在问题：第一次barrier没问题，但是有人还没离开barrier又看到flag=0一直睡死 (类比：一堆人跑到了终点线睡觉，最后到的一个不睡觉，把flag的0翻成1，所有人看到1之后又起跑，第一个跑到下一终点线的把1重置回0，但是这时候可能有人还没起床，永远看不到flag变成1睡死在那里)</p><p>解决方法：增加leave_counter</p><p><strong>sense reversal</strong>: one spin instead of two. 不要等待flag变成某个固定值 (如 1)，而是等待flag的值发生改变 (change)。利用 flag 的两个状态（0 和 1）来区分连续的barrier。每个线程需要维护local sense，记录它期望在当前屏障实例中看到的flag值</p><h1 id="lec-22-lock-free"><a class="markdownIt-Anchor" href="#lec-22-lock-free"></a> Lec 22. Lock-free</h1><p>Problem: lock can be big and expensive</p><p>C++11 <code>atomic&lt;T&gt;</code>: 使用mutex或处理器硬件原子指令(如果T是基本类型). 使用<code>.is_lock_free()</code> 查看是否atomicity的implementation是无锁的</p><p>Linkedlist例子</p><ul><li>global lock</li><li><strong>fine-grained lock</strong><ul><li>Challenge: deadlock? livelock?</li><li>Costs: overhead of locks, extra storage cost</li></ul></li></ul><h2 id="lock-free-data-structures"><a class="markdownIt-Anchor" href="#lock-free-data-structures"></a> Lock-free data structures</h2><p><strong>Blocking</strong> algorithms/data structures: 一个线程能够防止其它线程完成操作. 只要算法使用了locks就是blocking的，无论锁的实现是spinning还是pre-emption.</p><p><strong>Non-blocking</strong> algorithms are <strong>lock-free</strong> if: some thread is guaranteed to make progress</p><h1 id="lec-23-transactional-memory"><a class="markdownIt-Anchor" href="#lec-23-transactional-memory"></a> Lec 23. Transactional Memory</h1><p>Atomic construct is <strong>declarative</strong>: programmer states what to do (保证一块区域的原子性), not how to do.</p><p><strong>Transaction memory ™</strong>: sequence of mem operations. Inspired by DB.</p><ul><li>Atomicity: All (if commits) or nothing (if aborts).</li><li>Isolation: Commit之前别的processor都看不到任何写入操作</li><li>Serializability: 与某个顺序执行的结果一致</li></ul><p><strong>Load-linked, store conditional (LL/SC)</strong>: lite version of TM</p><ul><li>pair of instructions (而不是一条原子指令，不像CAS)</li><li><code>load_linked(x)</code> 加载内存地址, <code>store_conditional</code> 如果x在上次LL指令之后没有被写入过，则写入内存</li></ul><p><strong>Failure</strong> atomic: locks （发生exceptions时会发生什么）</p><ul><li>programmer需要决定如何写undo code</li><li>undo code之前也有可能被别人看到错误的中间值</li></ul><p>使用transactions则会由系统自动管理这些exceptions</p><p><strong>Composability</strong>:</p><p>e.g. 函数 <code>transfer(A,B,amount) &#123;lock(A), lock(B), ...&#125;</code></p><p>如果 <code>transfer(x,y,100)</code> 和 <code>transfer(y,x,100)</code> 同时运行会死锁</p><p>Coarse-grained lock: poor performance</p><p>Fine-grain lock: good performance, but may have deadlock</p><p><strong>Advantages (promise) of TM</strong>:</p><ul><li><p>easy to use synchronization construct</p><p>programmer如果要正确实现synchronization比较复杂，但transactional memory用起来和corase-grain lock一样简单</p></li><li><p>often performs as well as fine-grained locks</p><p>自动实现read-read序列化和fine-grained concurrency. Performance较好，易于开发</p></li><li><p>failure atomicity and recovery, composability (不用担心failure scenerios)</p></li></ul><p>Atomic != lock+unlock</p><ul><li>atomic: 只是一个high-level declaration, 没有决定如何实现.</li><li>Lock可以用来实现atomic，也可以用于除了atomicity以外的用途</li><li>programmer mistake：如果atomic的区域被错误分割成了两块atomic则中间可能被别人修改</li></ul><h2 id="implementing-transactional-memory-tm"><a class="markdownIt-Anchor" href="#implementing-transactional-memory-tm"></a> Implementing Transactional Memory ™</h2><p>TM需要提供atomicity和isolation.</p><p>Basic implementation: 需要data versioning（来支持回滚）；冲突检测和解决机制（什么时候abort）</p><p>分为硬件TM(<strong>HTM</strong>)，软件TM(<strong>STM</strong>)和<strong>Hybrid TM</strong> （例如硬件加速的STM）</p><h3 id="data-versioning"><a class="markdownIt-Anchor" href="#data-versioning"></a> <strong>Data Versioning</strong></h3><ul><li><strong>eager versioning</strong> (undo-log based): 立即更新内存，保存undo log以备abort用<ul><li><strong>faster commit, slower aborts</strong>, <strong>fault tolerance issues</strong> (transaction中间发生crash)</li></ul></li><li><strong>lazy versioning</strong> (write-buffer based): 只写入到write buffer内，在commit成功时flush buffer<ul><li><strong>faster abort, slower commit</strong>, no fault tolerance issues</li></ul></li></ul><h3 id="conflict-detection"><a class="markdownIt-Anchor" href="#conflict-detection"></a> <strong>Conflict detection</strong></h3><ul><li><p>read-write conflict: A读X，B有一个pending transaction写入X</p></li><li><p>write-write conflict: A和B都在pending且都想写入X</p></li><li><p>系统需要追踪一个transaction的read set和write set</p></li><li><p><strong>pessimistic detection</strong>: load和store时检测conflict</p><ul><li>good: detect conflicts early (更少的undo, 一部分abort会转为stall)</li><li>bad: no progress guarantee, fine-grained communication, detection on critical path</li></ul></li><li><p><strong>optimistic detection</strong>: 当尝试commit的时候检测conflict（使用硬件cache coherence检测）。有冲突时committing transaction有优先权</p><ul><li>good: forward progress guarantees; bulk communication &amp; conflict detection</li><li>bad: detects conflicts late; fairness problems</li></ul></li></ul><h3 id="hardware-transactional-memory-htm"><a class="markdownIt-Anchor" href="#hardware-transactional-memory-htm"></a> Hardware transactional memory (HTM)</h3><ul><li><p><strong>data versioning</strong> is implemented in <strong>caches</strong> (cache the write buffer or the undo log)</p><ul><li>一条cache line有MESI state, 和R bit, W bit (read set, write set). 此处的R和W可以选择word或cache-line作为granularity单位</li><li>例如intel haswell: 有硬件指令 xbegin, xend, xabort. Xbegin将指针设置为fallback address, 在abort发生时回退指针。注意progress is not guaranteed (even if no other threads), 因为处理有很多原因会abort，包括cache line被evict。</li></ul><p>TSX doesn’t guarantee progress</p><ul><li>transaction有很多原因会失败。fallback path仍然需要锁。</li><li>TSX只能跟踪有限数量的内存位置(minimize memory touched).</li></ul></li></ul><h1 id="lec-25-parallel-dnn"><a class="markdownIt-Anchor" href="#lec-25-parallel-dnn"></a> Lec 25. Parallel DNN</h1><h2 id="parallel-deep-neural-networks"><a class="markdownIt-Anchor" href="#parallel-deep-neural-networks"></a> Parallel Deep Neural Networks</h2><ul><li><p>CNN (Convolutional neural networks) 卷积</p><p>有一堆卷积的filter, 同时对image应用这么多卷积核</p><p>卷积可以用矩阵乘法来算（卷积核视为一维数组，每个卷积核代表一列）</p><p>普通的3层for循环的dense matrix multiplication的arithmetic intensity很低</p><p>改为blocked dense matrix multiplication会更好（计算量不变，缓存命中率更好）</p></li><li><p>reducing network footprint</p><p>例如VGG，末尾的几个全连接层因为fully connected所以很大，model parameters太多会造成负担</p><p>压缩network的方法</p><ul><li><p><strong>prune low-weight links</strong>: model训练完之后会有很多0和接近0的值，90%的值拿掉不会显著影响精度；将参数矩阵存为compressed sparse row (CSR)格式</p></li><li><p><strong>weight sharing</strong>: 相近的数值可以合并，让所有参数共享a small set of weights</p><p>将weights (32-bit浮点)变为<strong>cluster index</strong>(2-bit 代号)和<strong>centroids</strong>(2-bit代号到浮点数值的映射表)</p></li><li><p><strong>huffman encode</strong> quantized weights and CSR indices</p></li></ul></li></ul><p>上面说的是evaluation, 下面说training</p><p>training比evaluation明显更耗资源</p><h2 id="data-parallel-training"><a class="markdownIt-Anchor" href="#data-parallel-training"></a> <strong>data-parallel training</strong></h2><ul><li><p>across images: 每个核心有完整的model的copy. 两个核心对不同的image做训练，加barrier把梯度相加</p></li><li><p>Asynchronous parameter update: 引入<strong>parameter server</strong></p><p>首先parameter server向大家分发parameter values, 同时divide training image set (让每个node拿到完整model和一部分训练集），如果有节点训练完成则向parameter server发送subgradient，让server的参数得到更新</p><p>avoid global sync on all parameter updates between each SGD iteration</p><p>好处: <strong>没有barrier</strong>；坏处：loss不好，影响SGD的收敛，实测效果不好、</p><p>parameter server会成为瓶颈 ==&gt; 通过将parameter server分开来可以缓解</p></li></ul><h2 id="model-parallelism"><a class="markdownIt-Anchor" href="#model-parallelism"></a> <strong>model parallelism</strong></h2><ul><li>如果一个节点放不下model如何处理？</li></ul><h1 id="lec-26-parallel-dnn-part-2"><a class="markdownIt-Anchor" href="#lec-26-parallel-dnn-part-2"></a> Lec 26. Parallel DNN part 2</h1><h2 id="allreduce"><a class="markdownIt-Anchor" href="#allreduce"></a> AllReduce</h2><p>Allreduce: perform element-wise reduction across multiple devices</p><p>例如：有四块GPU，每块GPU有一部分result，现在要把这四块result加起来并且让所有人都拿到最终结果</p><ul><li><p>(Parameter Server): <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>∗</mo><mi>N</mi><mo>∗</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">2*N*M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span></p></li><li><p><strong>Naive AllReduce</strong>: 每个worker发送自己的gradient给其它所有邻居</p><p>假设有N个worker, 每个worker有M个参数, 那么总共需要发送 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>∗</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">N*(N-1)*M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> 个parameter</p><p>communication复杂度很高</p></li><li><p><strong>Ring AllReduce</strong>: 将M个参数分成N个slice，分<strong>aggregation</strong>和<strong>broadcast</strong>两步</p><p>aggregation: 同一时刻，每个worker发送一个slice给别人，收到别人的slice后加起来，重复N次</p><p>e.g. A发a0给B，B发b1给C…</p><p>broadcast: 每个worker发送一个aggregated slice给别人，重复N次</p><p>总的communication需要发送 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>∗</mo><mi>M</mi><mo>∗</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">2*M*N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> 个parameter，其中aggreation和broadcast各占<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>∗</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">M*N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span></p></li><li><p><strong>Tree AllReduce</strong>: 树状结构，同样分aggregation和broadcast两步</p><p>aggregation: 每人发M个参数给parent，重复log N次(tree height)</p><p>broadcast: 每人发送M个参数给所有的children，重复log N次</p><p>Overall communication同上</p></li><li><p><strong>Butterfly AllReduce</strong>: 使用butterfly network。重复logN次</p><p>Overall communication为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>∗</mo><mi>M</mi><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N*M*\log(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mclose">)</span></span></span></span></p></li></ul><p>同样是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>∗</mo><mi>M</mi><mo>∗</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">2*M*N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span>, Ring Allreduce表现比parameter server和tree allreduce都更好</p><ul><li><p>Ring的latency最低，分配任务最平均，scale well</p><p>每个worker每个iteration发送M/N个参数，重复2N个iteration，latency = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi mathvariant="normal">/</mi><mi>N</mi><mo>∗</mo><mo stretchy="false">(</mo><mn>2</mn><mo>∗</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M/N*(2*N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mclose">)</span></span></span></span></p><p>在N变得很大的时候，latency为2M，N消失了，代表latency和机器数量无关</p><p>通俗理解为，在机器数量变多的时候，因为slice变小，需要发送的量也会变小</p></li></ul><h2 id="model-parallelism-2"><a class="markdownIt-Anchor" href="#model-parallelism-2"></a> Model Parallelism</h2><p>如果一个节点放不下model如何处理？</p><p>将一个model split开来放在不同的GPU上做</p><p>不同的运算可以选择使用data parallelism或model parallelism</p><p><strong>Convolutional layers</strong>: 大量计算时长占比，少量parameters，大量中间结果 ==&gt; Data parallelism</p><p><strong>Fully-connected layers</strong>: 少量计算时长占比，大量parameters（不想频繁为了parameters通信） ==&gt; Model parallelism</p><h2 id="pipeline-parallelism"><a class="markdownIt-Anchor" href="#pipeline-parallelism"></a> Pipeline parallelism</h2><p>因为放在了不同的GPU上运行导致出现三角形空隙，所以pipeline（pipeline无法彻底解决空隙问题，只能缓解）</p><hr><h1 id="一些基础知识"><a class="markdownIt-Anchor" href="#一些基础知识"></a> 一些基础知识</h1><h2 id="ispc"><a class="markdownIt-Anchor" href="#ispc"></a> ISPC</h2><p>ISPC代码调用时会生成多个program instances, 可以利用 <code>programCount</code> 和 <code>programIndex</code> 来获取instance总数和当前instance编号。</p><p><code>uniform</code> 表示在一个SIMD程序块中，变量对所有SIMD通道都是相同的值。仅仅是一种优化，不影响正确性(因为uniform变量只需要加载一次或执行一次，编译器可以做出优化，不加uniform可能造成不必要的重复计算)。</p><p>非uniform (<code>varying</code>) 表示变量在不同SIMD通道可能有不同的值。</p><p>所以说 <code>programCount</code> 是 uniform, <code>programIndex</code> 是 varying.</p><hr><p>ISPC可以通过tasks来实现多核加速，利用多线程。</p><p>Contrary to threads, tasks do not have execution context and they are only pieces of work. ISPC编译器接受tasks并自行决定启动多少个threads。</p><p>通常我们应该启动比cpu逻辑线程数更多的tasks数量，但也不要太多，否则会有scheduling的overhead。</p><p>task自带 <code>taskIndex</code>。</p><h2 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h2><p>host是CPU, device是GPU</p><p><code>__device__</code>: 在device上执行，只能在device中调用</p><p><code>__global__</code>: 在device上执行，只能在host中调用。叫做<strong>kernel</strong>，返回值必须是void</p><p><code>__host__</code>: 在host上执行且只能在host上调用</p><p><code>cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost)</code></p><hr><h3 id="threads-blocks-grids"><a class="markdownIt-Anchor" href="#threads-blocks-grids"></a> Threads, Blocks, Grids</h3><p>threads grouped into blocks</p><p>需要指明blocks的数量，和每个block中threads的数量。</p><p>假设n是总的threads数量, t是每个block中threads的数量。</p><p><code>KernelFunction&lt;&lt;&lt;ceil(n/t), t&gt;&gt;&gt;(args)</code></p><p>每一个thread都会运行同样的kernel，每一个thread由blockID和这个block中的threadID来标识。</p><hr><p>Example:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">vecAddKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> i = threadId.x + blockDim.x * blockId.x;<br>    <span class="hljs-keyword">if</span> (i&lt;n) C[i] = A[i] + B[i];<br>&#125;<br><span class="hljs-type">void</span> <span class="hljs-title function_">vecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> size = n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-type">float</span> *d_A, *d_B, *d_C;<br>  <br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_A, size);<br>    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_B, size);<br>    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_C, size);<br>  <br>    vecAddKernel&lt;&lt;&lt;<span class="hljs-built_in">ceil</span>(n/<span class="hljs-number">256</span>), <span class="hljs-number">256</span>&gt;&gt;&gt;(d_A, d_B, d_C, n);<br>    <br>  	cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);<br>    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);<br>&#125;<br></code></pre></td></tr></table></figure><p>注: 为什么<code>cudaMalloc</code>第一个参数是二级指针，而不直接使用返回值来赋值给指针？</p><p>因为 <code>cudaMalloc</code> 的返回值已经用来返回 <code>cudaError_t</code>。</p><hr><p>grid和blocks可以是1D, 2D, 3D的。上面这个例子是1D，所以是&quot;<code>.x</code>&quot;</p><p>2D的例子：假设要把一个WIDTH x WIDTH的矩阵P分成几块。</p><p>WIDTH=8, TILE_WIDTH为2的话，就是把8x8的矩阵分成16个小块(grid)，每一个小块大小是2x2(4个thread)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">dim3 <span class="hljs-title function_">dimGrid</span><span class="hljs-params">(WIDTH / TILE_WIDTH, WIDTH / TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>dim3 <span class="hljs-title function_">dimBlock</span><span class="hljs-params">(TILE_WIDTH, TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>MatrixMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(args);<br></code></pre></td></tr></table></figure><p>每一个thread可以用以下方式来标识</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">row    = blockId.y * blockDim.y + threadId.y;<br>column = blockId.x * blockDim.x + threadId.x;<br></code></pre></td></tr></table></figure><hr><p>为什么用两层threads？因为组成多个grid的thread blocks比一个很大的单个thread block更好管理。</p><p>GPU有很多很多核心，核心group成SM(streaming multiprocessors)，每一组SM有自己的内存和调度。</p><p>GPU不同时启动所有100万个threads，而是把大约1000个thread装进一个block里，并分发给SM。</p><p>assign给SM的thread block会使用SM的资源（寄存器和共享内存）。这些资源已经pre-allocated，且由于寄存器数量很多，在切换threads时不需要register flush。</p><hr><p>不同的block可以用任何顺序运行，因此不能assume block2在block1之后运行。如果真的要这么做，需要放在不同的kernel里（启动kernel比较耗资源）</p><p>同一个block中的thread可以使用 <code>__syncthreads()</code> 来做barrier synchronization。</p><p>但是通常不建议使用 <code>__syncthreads()</code></p><hr><p>如何选择合适的block size？</p><ul><li>Consideration 1: hardware constraints<ul><li>例如：每一个SM分配小于1536个thread，小于8个block；每一个block小于512个thread</li></ul></li><li>Consideration 2: complexity of each thread</li><li>Consideration 3: thread work imbalance.</li></ul><hr><p>GPU memory</p><p>Global memory很慢，所以同时运行大量线程，线程因为内存IO卡住的时候切换其它线程，这是massive multi-threading (MMT).</p><p>这样总的throughput很高，即使每个thread的延迟也很高。</p><p>每个SM有自己的scheduler，每个SM存储了所有thread的context(PC, reg等)，所以SM内能做到零开销线程切换。同时，SM scheduler有一个scoreboard追踪哪些thread是blocked/unblocked，所以SM有大约30个核但可以运行大约1000个线程。</p><hr><p>Tiled MM是一种进行矩阵乘法 内存友好的方法。</p><p>CUDA类型关键词</p><ul><li><code>__device__ __shared__</code> memory: shared; scope: block; lifetime: block</li><li><code>__device__</code> memory: global; scope: grid; lifetime: application</li><li><code>__device__ __constant__</code> memory: constant; scope: grid; lifetime: application</li></ul><hr><p>Race conditions:</p><p>CUDA中难以实现mutex，而且包含critical sections的代码在GPU上本来就运行得不好。</p><p>CUDA中有一些原子操作，可以在global或shared memory变量上操作</p><ul><li><code>int atomicInc(int *addr)</code>: 加一，返回旧值</li><li><code>int atomicAdd(int *addr, int val)</code>: 加val, 返回旧值</li><li><code>int atomicMax(int *addr, int val)</code>: 让*addr=max(*addr, val) 并返回旧值</li><li><code>int atomicExch(int *addr1, int val)</code>: set</li><li><code>int atomicCAS(int *addr, old, new)</code>: Compare and swap.<ul><li><code>if (*addr == old) *addr = new;</code></li></ul></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Lecture-Notes/" class="category-chain-item">Lecture Notes</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/C/" class="print-no-link">#C</a> <a href="/tags/Lecture-Notes/" class="print-no-link">#Lecture Notes</a> <a href="/tags/CUDA/" class="print-no-link">#CUDA</a></div></div><div class="license-box my-3"><div class="license-title"><div>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</div><div>https://www.billhu.us/2025/054_cmu_15618/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>Bill Hu</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>January 7, 2025</div></div><div class="license-meta-item"><div>Licensed under</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/057_cmu_17514/" title="[Lecture Notes] CMU 17-514 Software Construction"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">[Lecture Notes] CMU 17-514 Software Construction</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2024/051-cmu-15513/" title="[Lecture Notes] CMU 15-513 Intro to Computer Systems"><span class="hidden-mobile">[Lecture Notes] CMU 15-513 Intro to Computer Systems</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Table of Contents</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <span>With </span><a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-love"></i> <a href="/about" target="_blank" rel="nofollow noopener"><span>Bill Hu</span></a><div style="font-size:.85rem"><span id="timeDate">Loading days...</span> <span id="times">Loading time...</span><script src="/js/duration.min.js"></script></div></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,t=t.getElementById("subtitle");t&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript></body></html>