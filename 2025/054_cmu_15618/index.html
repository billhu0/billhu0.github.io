<!DOCTYPE html><html lang="en" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png"><link rel="icon" href="/img/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="Bill Hu"><meta name="keywords" content=""><meta name="description" content="CMU 15-618 Notes"><meta property="og:type" content="article"><meta property="og:title" content="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"><meta property="og:url" content="https://www.billhu.us/2025/054_cmu_15618/index.html"><meta property="og:site_name" content="Bill Hu&#39;s Blog"><meta property="og:description" content="CMU 15-618 Notes"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/ispc-sinx-interleave.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/hw-shared-address.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/NUMA.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/gather.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/parallel-steps.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/synchronous-send-recv.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/async-send-recv.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/pipelined-communication.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/change-grid-traversal-order.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/fusing-loops.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/sharing-data.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/4d-array.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/interconnect-terminology.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/direct-indirect-networks.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/topology-1.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/topology-2.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/write-through-invalidation.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/msi.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/MESI.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/MOESIF.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/multilevel-cache-inclusion.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/nvgpu-no-cache-coherence.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/simple-directories.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg1.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg2.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/directory-eg3.webp"><meta property="og:image" content="https://www.billhu.us/2025/054_cmu_15618/sparse-directories.webp"><meta property="article:published_time" content="2025-01-07T19:17:06.000Z"><meta property="article:modified_time" content="2025-02-25T01:26:22.077Z"><meta property="article:author" content="Bill Hu"><meta property="article:tag" content="C"><meta property="article:tag" content="Lecture Notes"><meta property="article:tag" content="CUDA"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://www.billhu.us/2025/054_cmu_15618/interleave-thread.webp"><title>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming - Bill Hu&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.15.6/katex.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/myfont.css"><link rel="stylesheet" href="/css/jetbrains-mono.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"www.billhu.us",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:25,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},gtag:null},search_path:"/local-search.xml",include_content_in_search:!0});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script>!function(t,e,n,c,a,r){t[n]=t[n]||function(){(t[n].q=t[n].q||[]).push(arguments)},(a=e.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/fjnxcr4gva",(r=e.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script><meta name="google-site-verification" content="IqNfw3GiMxuhw7kYoEdhMJoh3j99KHuGI9bw00hPn2c"><link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet"><meta name="generator" content="Hexo 7.1.1"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Bill Hu&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>Home</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>Archives</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>Categories</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>Tags</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>About</span></a></li><li class="nav-item"><a class="nav-link" href="https://www.billhu.xyz" target="_self"><i class="iconfont icon-code"></i> <span>Portfolio</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-01-07 19:17" pubdate>January 7, 2025 pm</time></span></div><div class="mt-1"></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</h1><div class="markdown-body"><h1 id="lec-2-a-modern-multi-core-processor"><a class="markdownIt-Anchor" href="#lec-2-a-modern-multi-core-processor"></a> Lec 2. A modern multi-core processor</h1><p>4 key concepts: 两个与parallel execution有关, 两个与challenges of accessing memory 有关</p><h2 id="parallel-execution"><a class="markdownIt-Anchor" href="#parallel-execution"></a> Parallel execution</h2><ul><li><strong>Superscalar processor</strong>: Instruction level parallelism (ILP)<ul><li>ILP读未来的指令（每个周期读两条指令），有两个fetch/decode单元和两个exec单元，能够同时执行两条指令</li></ul></li><li><strong>Multi-core</strong>: 多个processing cores<ul><li>多核之前，处理器提升重点在更大缓存，更多分支预测predictor等；同时更多晶体管（才能放得下更多缓存和更多predictor和乱序执行逻辑）促生更小的晶体管，促进更高的计算机主频</li><li>2004年多核出现之后，人们在一个chip上放多个processor，用更多晶体管放更多核心。</li></ul></li><li><strong>SIMD processing</strong> (aka <strong>Vector processing</strong>): 多个ALU(同一个core内)<ul><li>仍然只需要一个fetch/decode单元，多个ALU。</li><li>conditional execution: 如果想simd的程序块有if else，要通过mask处理</li><li>手写avx代码（cpu指令）是<strong>explicit SIMD</strong>; 而GPU是<strong>implicit SIMD</strong>，因为compiler生成的并不是并行指令（是普通的scalar instructions），只有GPU硬件运行才是SIMD的</li></ul></li></ul><h2 id="accessing-memory"><a class="markdownIt-Anchor" href="#accessing-memory"></a> Accessing memory</h2><ul><li><p><strong>cache</strong>: <strong>reduce latency</strong></p></li><li><p><strong>prefetching</strong> reduces stalls: <strong>hides latency</strong></p></li><li><p><strong>Multi-threading</strong>, <strong>interleave</strong> processing of multiple threads</p><ul><li>跟prefetching一样，也是hide latency，不能reduce latency</li><li>指的是：开多个线程，在一个线程卡住的时候执行别的线程</li><li>在下图中，创建thread1的时候不仅仅创建thread1，还会告诉电脑创建了thread 2 3 4，硬件检测线程是否发生了stall（被等待内存操作卡住），如果发生了stall会很快切换到别的线程，想juggling一样。硬件决定如何juggle这些线程。</li><li>这样memory latency仍然存在，但是被hide了。memory latency在后台发生，前台CPU一直在执行有用的工作。</li><li>这种操作会导致单个线程的执行时间变长（因为thread1从runnable到重新开始执行有一段空挡（这段空隙在执行thread 2 3 4）。</li><li>需要更多硬件资源，存储线程的register等状态信息，这样切换线程才会快。且需要较大的memory bandwidth。</li></ul></li></ul><img src="/2025/054_cmu_15618/interleave-thread.webp" srcset="/img/loading.gif" lazyload><p>GPU设计成处理大量数据（远大于核内缓存的数据量）。</p><p>与CPU内存相比，GPU显存带宽更高，但延迟更高。</p><h2 id="关于nv-gpu架构的一些备注"><a class="markdownIt-Anchor" href="#关于nv-gpu架构的一些备注"></a> 关于NV GPU架构的一些备注</h2><p>一个GPU有多个SM；每个SM上有一块shared memory；同时每个线程有自己的registers和local memory，但如果线程registers超出，则会溢出到global memory。</p><p>因为一个block保证在一个SM上，所以threads within a block可以访问__shared__ memory。</p><p>Warp是硬件规定好的，通常为32-threads，warp内一定所有线程在任意时刻都做同一件事情（如果遇到branch divergence，则会被mask out，但warp仍然同一时刻执行同一个branch）</p><p>假设block size是256（程序员指定的），warp为32（硬件规定），那么threads in the same block可能在不同的warp里，threads within a block可能同一时刻在执行不同的指令，因为它们在不同的warp调度不同。</p><h1 id="lec-3-progpramming-models"><a class="markdownIt-Anchor" href="#lec-3-progpramming-models"></a> Lec 3. Progpramming Models</h1><h2 id="abstraction-vs-implementation"><a class="markdownIt-Anchor" href="#abstraction-vs-implementation"></a> Abstraction vs Implementation</h2><p>abstraction和implementation不是一个东西！</p><p><strong>ISPC</strong> (Intel SPMD Program Compiler)</p><p>SPMD: single program multiple data</p><p>一个ISPC计算sin(x)的例子</p><ul><li><p>Interleaved</p><ul><li><img src="/2025/054_cmu_15618/ispc-sinx-interleave.webp" srcset="/img/loading.gif" lazyload></li><li><pre class="highlight"><code class="c">export <span class="hljs-type">void</span> <span class="hljs-title function_">sinx</span><span class="hljs-params">(
  uniform <span class="hljs-type">int</span> N, uniform <span class="hljs-type">int</span> terms,
  uniform <span class="hljs-type">float</span>* x, uniform <span class="hljs-type">float</span>* result)</span> &#123;
  <span class="hljs-comment">// assume N % programCount = 0</span>
  <span class="hljs-keyword">for</span> (uniform <span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;N; i+=programCount) &#123;
    <span class="hljs-type">int</span> idx = i + programIndex;
    <span class="hljs-comment">// 不重要</span>
  &#125;
&#125;
&lt;!--code￼<span class="hljs-number">0</span>--&gt;

</code></pre></li></ul></li></ul><p>在这个示例中blocked比interleaved更好。</p><p>因为每个iteration工作量完全相同，SIMD指令load连续内存（直接_mm_load_ps1）比load不连续内存(这种操作叫gather，只在AVX2及以后才支持）更快。</p><p>我们可以使用<code>foreach</code>来代替。</p><p><code>foreach</code>表示循环中的每一次iteration都是独立的，由ISPC决定如何分配</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">foreach(i = <span class="hljs-number">0</span> ... N) &#123;<br>	<span class="hljs-comment">// index ...</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>ISPC: abstraction VS. Implementation<ul><li>Programming model: SPMD<ul><li>程序员想的是，我们有programCount个逻辑指令流，写代码也按照这样的abstration去写</li></ul></li><li>Implementation: SIMD<ul><li>ISPC输出SSE4或AVX这种指令来实现逻辑操作</li></ul></li></ul></li></ul><h2 id="four-models-of-communication"><a class="markdownIt-Anchor" href="#four-models-of-communication"></a> Four Models of Communication</h2><ul><li><p><strong>Shared Address Space</strong> model</p><ul><li><p>共享内存，不同线程通过读写同一块内存来通信</p></li><li><p>很多人认为这是最方便易用的model，因为这和sequential programming最相近</p></li><li><p>每个处理器都能访问任意内存地址</p></li><li><p><strong>Uniform memory access</strong> time; Symmetric shared-memory multi-processor (SMP), : 指每个处理器都能访问内存且访问内存所需时间一致。<strong>Scalability不好。</strong></p><img src="/2025/054_cmu_15618/hw-shared-address.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>Non-uniform memory access (NUMA)</strong>: 每个处理器都能访问完整内存，但所需时间不一致。这样<strong>Scalability</strong>比较好。但是performance tuning可能需要更多精力。<img src="/2025/054_cmu_15618/NUMA.webp" srcset="/img/loading.gif" lazyload></p></li></ul></li><li><p><strong>Message Passing</strong> model</p><ul><li>线程之间不共享内存（address space不共享），只能通过发送和接收messages通信</li><li>相比shared memory的优点：不需要别的硬件，可以纯软件做，实现起来简单。常用的库是<strong>MPI</strong> (message passing interface)</li><li>现代很常用的操作是，在一个节点（节点内是多核的）内用shared address，在不同节点间用message passing</li></ul></li></ul><div class="note note-info"><p>这里很搞：abstraction can target different types of machines.</p><p>分清<strong>abstraction</strong>和<strong>implementation</strong>的区别！</p><p>比如说：</p><p>message passing 这个 <strong>abstraction</strong> 可以使用硬件上的 shared address space 来 <strong>implement</strong>。</p><ul><li>发送/接收消息就是读写message library的buffer。</li></ul><p>shared address space 这个 <strong>abstraction</strong> 在不支持硬件 shared address space 的机器上也可以用软件来 <strong>implement</strong> (但是低效)</p><ul><li>所有涉及共享变量的page都标记成invalid，然后使用page-fault handler来处理网络请求</li></ul></div><ul><li><p><strong>The data-parallel</strong> model</p><ul><li>上面的shared address space和message passing更general</li><li>data-parallel更specialized, rigid。</li><li>在不同的数据（数据块）上执行相同的操作。</li><li>通常用SPMD的形式：<strong>map (function, collection)</strong>，其中对所有的数据都做function的操作，function可以是很长的一段逻辑，比如一个loop body。collection是一组数据，可以包含多个数据。</li><li><strong>gather/scatter</strong>: gather是把本来不连续的数据按照index放到一起，scatter是把本来连续的数据分散开。<img src="/2025/054_cmu_15618/gather.webp" srcset="/img/loading.gif" lazyload></li></ul></li><li><p><strong>The systolic arrays</strong> model</p><ul><li><p>读内存太慢了，memory bandwidth会成为瓶颈</p></li><li><p>所以要<strong>避免不必要的内存读取</strong>，尽量只读一次内存，做完所有需要用到这块内存的操作，再写回去。</p></li><li><p>示例：矩阵乘法 <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2VrnkXd9QR8">https://www.youtube.com/watch?v=2VrnkXd9QR8</a></p></li></ul></li></ul><p>概括four models of communication</p><ul><li>shared address space: 共享内存通信</li><li>message passing: 发消息通信</li><li>data parallel: 对一大块数据分块执行相同操作</li><li>systolic arrays: 减少内存读取</li></ul><h1 id="lec-5-parallel-programming-basics"><a class="markdownIt-Anchor" href="#lec-5-parallel-programming-basics"></a> Lec 5. Parallel Programming Basics</h1><img src="/2025/054_cmu_15618/parallel-steps.webp" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Decomposition</strong></p><p>把问题分解成 <strong>tasks</strong></p><p>Main idea: 创造至少能把机器占满的tasks数量，通常对于t个processor会给多于t个task，并且要让这些task尽可能 <strong>independent</strong>.</p><p><strong>Amdahl’s Law</strong>: 程序中有S部分只能顺序运行（无法用并行加速）则整个程序的speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo></mrow><annotation encoding="application/x-tex">\leq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7719400000000001em;vertical-align:-.13597em"></span><span class="mrel">≤</span></span></span></span> 1/S.</p><p>通常是programmer负责decomposition。</p></li><li><p><strong>Assignment</strong></p><p>Goal: <strong>balance workload, reduce communication costs</strong></p><p>can be performed <strong>statically</strong> or <strong>dynamically</strong></p><ul><li>statically: e.g. ISPC <code>foreach</code></li><li>dynamically: e.g. ISPC <code>launch tasks</code> 运行的时候会维护线程池，线程池中的线程从任务队列中读。这样做的优点是runtime workload balance.</li></ul></li><li><p><strong>Orchestration</strong></p><p>Goal: <strong>reduce communication/sync cost</strong>, preserve locality of data reference, reduce overhead</p><p>需要考虑机器的特性（上面的decomposition和assignment不用太考虑）。</p><p>包括</p><ul><li><strong>structuring communication</strong>: 信息传递模型 e.g. 传一个chunk数据而不是只传一个byte，节约overhead</li><li><strong>adding synchronization</strong> to preserve dependencies</li><li>organizing <strong>data structures</strong> in memory</li><li><strong>scheduling</strong> tasks</li></ul></li><li><p><strong>Mapping</strong> to hardware</p><p>对程序员来说是optional的。programmer可以显式制定哪个thread跑在哪个processor上。</p><ul><li>mapping by <strong>OS</strong>: e.g. pthread</li><li>mapping by <strong>compiler</strong>: e.g. ISPC maps ISPC program instances to vector instruction lanes</li><li>mapping by <strong>hardware</strong> e.g. GPU map CUDA threads to GPU cores</li></ul><p>Mapping 还能有不同的decisions，比如说</p><ul><li>Place <strong>related</strong> threads on the same processor: 最大化locality，共享数据，减少通讯成本</li><li>Place <strong>unrelated</strong> threads on the same processor: 可能一个thread受制于内存带宽，另一个受制于计算时间，这两个thread放在一起可以让处理器利用率更高</li></ul></li></ul><p>A parallel programming example: 2D-grid based solver</p><p>TODO here.</p><h1 id="lec-6-work-distribution-scheduling"><a class="markdownIt-Anchor" href="#lec-6-work-distribution-scheduling"></a> Lec 6. Work Distribution &amp; Scheduling</h1><p>key goals:</p><ul><li>balance workload</li><li>reduce communication</li><li>reduce extra work (overhead)</li></ul><h2 id="workload-balance"><a class="markdownIt-Anchor" href="#workload-balance"></a> workload balance</h2><ul><li><p><strong>Static assignment</strong></p><p>任务分配在运行之前就已经 <strong>pre-determined</strong></p><p>例如之前讲的blocked assignment和interleaved assignment.</p><p><strong>Zero runtime overhead</strong></p><p>当任务量可预测时可以使用。不一定要任务量相同，可预测不会变就行。</p></li><li><p><strong>Semi-static assignment</strong></p><p>可预测未来短期内的任务量</p><p>一边运行一边profile并调整任务分配（periodically profiles itself and re-adjusts assignment）</p></li><li><p><strong>Dynamic assignment</strong></p><p>任务量unpredictable.</p><ul><li><pre class="highlight"><code class="c"><span class="hljs-keyword">while</span> (<span class="hljs-number">1</span>) &#123;
  <span class="hljs-type">int</span> i;
  lock(counter_lock);
  i = counter++;
  unlock(counter_lock);  <span class="hljs-comment">// 或使用 atomic_incr(counter); 代替</span>
  <span class="hljs-keyword">if</span> (i &gt;= N) <span class="hljs-keyword">break</span>;
  <span class="hljs-comment">// do with index i</span>
&#125;
&lt;!--code￼<span class="hljs-number">2</span>--&gt;

</code></pre></li></ul></li></ul><h3 id="scheuling-fork-join-programs"><a class="markdownIt-Anchor" href="#scheuling-fork-join-programs"></a> Scheuling fork-join programs</h3><ul><li><p>Bad idea: <code>cilk_spawn</code> --&gt; <code>pthread_create</code>, <code>cilk_sync</code> --&gt; <code>pthread_join</code></p><p>因为创建kernel thread开销很大。</p><p>应该用线程池。</p></li><li><p>让idle thread 从别家thread的queue里steal work to do.</p><p><strong>continuation first</strong>:</p><ul><li>record child for later execution</li><li>child is made available for stealing by other threads (child stealing)</li><li>在遇到spawn的时候，自己执行spawn后面的任务，并把spawn出来的放在自己的work queue里，等待别的线程（如果别的线程有空闲）steal自己的任务。</li><li>如果没有stealing，那么（相比于去除所有spawn语句）执行顺序全都是反的</li></ul><p><strong>child first</strong>:</p><ul><li>record continuation for later execution</li><li>continuation is made available for stealing by other threads (continuation stealing)</li><li>遇到spawn的时候，只创建一个可被steal的项目。</li></ul></li><li><p>work queue可以用<strong>dequeue</strong> (double-ended queue)实现</p><p>每一个线程有自己的work queue，针对自己的work queue，在尾部添加，从尾部取出</p><p>如果要steal别的线程的work queue，从头部取出</p></li></ul><h1 id="lec-7-locality-communication-and-contention"><a class="markdownIt-Anchor" href="#lec-7-locality-communication-and-contention"></a> Lec 7. Locality, Communication, and Contention</h1><p>Lec6讲如何平均分配任务，Lec7讲如何降低communication开销.</p><ul><li><p><strong>synchronous (blocking)</strong> send and receive</p><img src="/2025/054_cmu_15618/synchronous-send-recv.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>non-blocking asynchronous</strong> send and receive</p></li></ul><p>send()和recv()函数会立即返回 在后台做事</p><img src="/2025/054_cmu_15618/async-send-recv.webp" srcset="/img/loading.gif" lazyload><h2 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h2><p>使用Pipeline: <strong>Latency 不变, Throughput 增加</strong></p><p>例子：</p><ul><li><p>Communication = Overhead(橙色) + Occupancy (蓝色) + Network delay (灰色)</p></li><li><p>最长的部分是瓶颈，决定了throughput上限</p></li></ul><img src="/2025/054_cmu_15618/pipelined-communication.webp" srcset="/img/loading.gif" lazyload><ul><li><p><strong>Overlap</strong>: communication和其它工作同时运行的时间。</p><p>我们希望能尽可能增加overlap这样communication cost才会降低。</p><p>降低overlap的方法</p><ul><li>Example 1: <strong>Asynchronous</strong> message send/recv 异步消息</li><li>Example 2: <strong>Pipelining</strong> 发送多条消息时让这个发送过程overlap</li></ul></li></ul><h2 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h2><p>Communication包含inherent和artifactual</p><ul><li><p><strong>Inherent communication</strong>: 程序算法写好的，必须发生的通信</p><ul><li>Communication-to-computation ratio: 通信量/计算量 的比值。越低越好。</li><li><strong>arithmetic intensity</strong>: 1/communication-to-computation ratio. 越高越好。</li></ul></li><li><p><strong>Artifactual communication</strong>: 所有别的通信，因为memory hierarchy导致额外的通信，例如L1/L2/L3/内存/网络之间的通信。包括：</p><p>① 系统有<strong>minimum granularity of transfer</strong>: 即使只需要读取4byte数据，也需要复制64-byte整条cache line</p><p>② 系统有<strong>rules of operation</strong>: 例如，写入内存需要先把内存读到cache line中（write-allocate）之后踢出cache line再写入内存，导致一次写入操作需要访问两次内存</p><p>③ <strong>Poor placement</strong> of data in distributed memories: 被某个processor访问最多的数据并没有放在这个processor附近</p><p>④ Finite replication <strong>capacity</strong>: 因为cache太小放不下，会被踢掉，所以有一些数据频繁被踢出/放入cache</p><p>提高<strong>locality</strong>对降低artifactual communication很重要</p></li></ul><p>提高<strong>temporal locality</strong>的例子</p><ul><li><p>by changing grid traversal order</p><img src="/2025/054_cmu_15618/change-grid-traversal-order.webp" srcset="/img/loading.gif" lazyload></li><li><p>by fusing loops</p><img src="/2025/054_cmu_15618/fusing-loops.webp" srcset="/img/loading.gif" lazyload></li><li><p>by sharing data</p><img src="/2025/054_cmu_15618/sharing-data.webp" srcset="/img/loading.gif" lazyload></li></ul><p>提高<strong>spatial locality</strong></p><ul><li><p>false sharing 不好</p></li><li><p><strong>4D array layout</strong> (<strong>blocked data layout</strong>): Embedding a 2D array within another 2D array allows page granularities to remain within a tile, making it practical to map data to local portions of physical memory (thereby reducing cache miss latencies to main memory).</p><img src="/2025/054_cmu_15618/4d-array.webp" srcset="/img/loading.gif" lazyload></li></ul><h2 id="contention"><a class="markdownIt-Anchor" href="#contention"></a> Contention</h2><p><strong>Contention</strong>: 在短时间内很多人请求同一个resource</p><p>Example: distributed work queues (让每个线程有自己的work queue)可以降低contention</p><h2 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h2><ul><li>降低communication costs<ul><li><strong>Reduce overhead</strong>: 发更少的消息数量，更长的消息内容（合并短消息）</li><li><strong>Reduce delay</strong>：提高locality</li><li><strong>Reduce contention</strong>: 把contended resource分开，例如local copies, fine-grained locks</li><li><strong>Increase overlap</strong>: 用异步消息、pipeline等 提高communication和computation的overlap</li></ul></li></ul><h1 id="lecture-9-workload-driven-perf-evaluation"><a class="markdownIt-Anchor" href="#lecture-9-workload-driven-perf-evaluation"></a> Lecture 9. Workload-Driven Perf Evaluation</h1><ul><li>Super-linear Speedup:<ul><li>processor足够多的时候，每个processor分到的数据fits in cache</li></ul></li><li>Decreasing Speedup:<ul><li>随着processor增多，communication占比太大了</li></ul></li><li>Low speedup:<ul><li>Increasing contexts are hyperthreaded contexts (?)</li></ul></li></ul><p><strong>Resource-oriented</strong> scaling properties</p><ul><li><strong>Problem constrained</strong> scaling (PC)<ul><li>更快速解决同一个问题</li></ul></li><li><strong>Memory constrained</strong> scaling (MC)<ul><li>不爆内存的情况下运行最大能完成的任务</li></ul></li><li><strong>Time constrained</strong> scaling (TC)<ul><li>同样的时间内完成更多任务</li></ul></li></ul><h2 id="simulation"><a class="markdownIt-Anchor" href="#simulation"></a> Simulation</h2><p><strong>Execution-driven</strong> simulator</p><ul><li>模拟内存，模拟内存访问</li><li>模拟器的performance通常与模拟的细节数量成反比</li></ul><p><strong>Trace-driven</strong> simulator</p><ul><li>在real machine上运行real code得到内存访问的trace，或者用execution-driven simulator生成trace</li><li>然后在模拟器上运行trace</li></ul><h1 id="lec-10-interconnects"><a class="markdownIt-Anchor" href="#lec-10-interconnects"></a> Lec 10. Interconnects</h1><h2 id="interconnect-terminology"><a class="markdownIt-Anchor" href="#interconnect-terminology"></a> Interconnect terminology</h2><p>Terminology</p><ul><li><p><strong>Network node</strong>: 网络终端，会产生或消耗traffic，例如processor cache</p></li><li><p><strong>Network interface</strong>: 把nodes和network相连</p></li><li><p><strong>Switch/Router</strong>: 将固定数量的input links与固定数量的output links相连</p></li><li><p><strong>Link</strong>: 传输信号的线缆</p><img src="/2025/054_cmu_15618/interconnect-terminology.webp" srcset="/img/loading.gif" lazyload></li></ul><p>设计interconnection需要考虑的因素</p><ul><li><p><strong>topology</strong>: 怎么相连</p><ul><li><p>topology的属性：</p><ul><li><p><strong>routing distance</strong>: nodes之间的长度，nodes相连需要多少个links (hops)</p></li><li><p><strong>diameter</strong>: 最大routing distance</p></li><li><p><strong>average distance</strong>: 平均routing distance</p></li><li><p><strong>direct / indirect</strong> networks</p><img src="/2025/054_cmu_15618/direct-indirect-networks.webp" srcset="/img/loading.gif" lazyload></li><li><p><strong>bisection bandwidth</strong></p></li><li><p><strong>blocking vs non-blocking</strong>: 如果任何两个pairs of nodes可以同时传输，不相干扰，则为non-blocking。大部分network都是blocking的</p></li></ul></li></ul></li><li><p>routing: 消息沿什么路线传输到达目的地？可以static可以adaptive</p></li><li><p>buffering and flow control</p></li></ul><img src="/2025/054_cmu_15618/topology-1.webp" srcset="/img/loading.gif" lazyload> <img src="/2025/054_cmu_15618/topology-2.webp" srcset="/img/loading.gif" lazyload><h2 id="buffering-and-flow-control"><a class="markdownIt-Anchor" href="#buffering-and-flow-control"></a> Buffering and Flow control</h2><p>和14740的第一节课讲的很像</p><h1 id="lec-11-perf-tools"><a class="markdownIt-Anchor" href="#lec-11-perf-tools"></a> Lec 11. Perf Tools</h1><h2 id="性能测试工具"><a class="markdownIt-Anchor" href="#性能测试工具"></a> 性能测试工具</h2><p><strong>GProf</strong></p><ul><li>compiler flag <code>-pg</code></li><li>places a call into every function --&gt; <strong>call graph</strong> (total time in each function)</li><li>先跑程序，然后单独使用 <code>gprof</code> 命令(不传参数)</li></ul><p><strong>Perf</strong></p><ul><li>有硬件指令测量性能计数器：cache misses, branch mispredicts, IPC, …</li><li><code>perf stat</code> (同时只能开启4个counter)</li></ul><p>VTune</p><ul><li>similar to perf: analysis across counters</li><li>有图形界面和解析</li></ul><h2 id="debug工具"><a class="markdownIt-Anchor" href="#debug工具"></a> Debug工具</h2><p>Valgrind</p><ul><li>heavy-weight, 需要 shadowing</li><li>有大量的overhead，不要用它测试performance</li><li><code>valgrind --tool=memcheck</code></li></ul><p>Address Sanitizer</p><ul><li>GCC and LLVM support, 有编译器支持</li><li>overhead比valgrind小一些</li><li><code>-fsanitize=address</code></li></ul><h2 id="advanced-analysis"><a class="markdownIt-Anchor" href="#advanced-analysis"></a> Advanced analysis</h2><p>Pin (Pintool)</p><ul><li>acts as a virtual machine: reassembles instructions</li><li>can record every single instruction/block(无跳转)/trace(可能跨函数)</li></ul><p>Contech</p><ul><li>compiler-based (uses clang+LLVM)</li><li>record control flow, mem access, concurrency</li><li>traces analyzed AFTER collection</li></ul><h2 id="summary-questions"><a class="markdownIt-Anchor" href="#summary-questions"></a> Summary questions</h2><ul><li>Reproducible?<ul><li>Do you have a workload? Is the system stable? (Stable是说每次运行性能差距不能太大)</li></ul></li><li>Workload at full CPU?<ul><li>Other users using CPU? Does workload rely heavily on IO?</li><li>使用<code>time</code> / <code>top</code> 看cpu占用时长</li></ul></li><li>Is CPU time confined to a small number of functions?<ul><li>占用时长最长的函数？算法复杂度？</li><li><code>gprof</code> / <code>perf</code></li></ul></li><li>Is there a small quantity of hot functions?<ul><li><code>perf</code> / <code>VTune</code></li></ul></li></ul><h1 id="lec-12-snooping-based-cache-coherence"><a class="markdownIt-Anchor" href="#lec-12-snooping-based-cache-coherence"></a> Lec 12. Snooping-based Cache Coherence</h1><div class="note note-info"><p>Recap:</p><ul><li><strong>write-allocate</strong>: 如果写入的内存不在cache中，则需要先把memory读到cache中再写入cache<ul><li>write-allocate与write-through和write-back都可以配合使用，但<strong>通常write-allocate与write-back搭配</strong></li></ul></li><li><strong>no-write-allocate</strong>: 直接写内存</li><li><strong>write-through</strong>: 同时写入cache和memory</li><li><strong>write-back</strong>: 只写入cache，之后再flush to memory</li></ul></div><h2 id="memory-coherence"><a class="markdownIt-Anchor" href="#memory-coherence"></a> Memory coherence</h2><p>A memory system is <strong>coherent</strong> if:</p><ul><li><p>the results of a parallel program’s execution are such that for each memory location, there is a hypothetical <strong>serial order of all program operations</strong> (executed by all processors) to the location that is consistent with the results of execution.</p><p>与某一个serial的内存访问顺序的结果一致</p></li></ul><p>Said differently</p><p><strong>Definition</strong>: A memory system is <strong>coherent</strong> if</p><ul><li><strong>obeys program order</strong>: 一个processor先write再read一定读到新值</li><li><strong>write-propagation</strong>: P1先write，一段时间后(suffciently separated in time) P2再read，则P2一定读到新值。注意此处需要相隔多久并没有定义。</li><li><strong>write serialization</strong>: 两个processor写入同一个位置，则大家都必须agree on one order, 大家都同意这个顺序</li></ul><p>可以用软件或硬件的方法解决</p><p>软件解法：OS采用page fault来propagate writes</p><p>硬件解法：Snooping based (本节课), Directory-based (下节课)</p><h2 id="snooping"><a class="markdownIt-Anchor" href="#snooping"></a> Snooping</h2><p>Cache controllers monitor (<strong>snoop</strong>) memory operations.</p><p>任意一个processor修改cache都会broadcast通知其它所有人</p><p>现在cache controller不仅需要响应处理器，还需要响应其它cache的broadcast</p><p>Assume write-through:</p><img src="/2025/054_cmu_15618/write-through-invalidation.webp" srcset="/img/loading.gif" lazyload><p>Write-through is inefficient: 因为每次write操作都需要写入内存，也是因为此原因write-through不常见</p><h2 id="msi-write-back-invalidation-protocol"><a class="markdownIt-Anchor" href="#msi-write-back-invalidation-protocol"></a> MSI write-back invalidation protocol</h2><p>Cache line加上dirty bit，如果dirty bit=1，代表处理器拥有这条cache line的exclusive ownership (<strong>Dirty = Exclusive</strong>)</p><p>如果其它processor想读同一条cache line，能从具有exclusive ownership的processor的cache中读（owner is responsible for supplying the line to other processors)</p><p>Processor也只能写入M-state的cache line; Processor能随时修改M-state的cache line不用通知他人</p><p>Cache controller监听是否有别人想要exclusive access，如果有，则自己必须要invalidate</p><img src="/2025/054_cmu_15618/msi.webp" srcset="/img/loading.gif" lazyload><h2 id="mesi-invalidation-protocol"><a class="markdownIt-Anchor" href="#mesi-invalidation-protocol"></a> MESI invalidation protocol</h2><p>在MSI中read会有两种情况</p><ul><li>BusRd: 从I转成S，即使并没有真正shared</li><li>BusRdX: 从S转成M</li></ul><p>添加一个新的<strong>E-state</strong> (<strong>exclusive clean</strong>) 代表exclusive但尚未修改</p><p>读取时询问别的cache有没有这条cache line，如果别人也有则从I进S，否则从I进E</p><p>从E升级到M不需要通知他人</p><img src="/2025/054_cmu_15618/MESI.webp" srcset="/img/loading.gif" lazyload><h2 id="other-mesif-moesi"><a class="markdownIt-Anchor" href="#other-mesif-moesi"></a> Other (MESIF, MOESI)</h2><ul><li><strong>MESIF</strong>: F = Forward<ul><li>类似与MESI，但是在多个cache都shared的时候，有一个cache不在S而在F</li><li>F holded my most recent requester</li><li>F负责service miss（给别人提供数据），作为对比，MESI中所有S都会做出响应</li><li>I不能直接进入S，你要么进E(如果没人有), 要么进F(别人也有cache，但你是most recent requester所以你要负责给别的cache line提供data)。E和F随后有可能进入S。</li><li>Intel处理器用的是MESIF</li></ul></li><li><strong>MOESI</strong>: O = Owned but Not Exclusive<ul><li>在MESI中，从M转成S需要flush to memory，MOESI添加O-state，从M转成O但不flush内存</li><li>在O时，这条cache line负责给别人提供data，但不flush进内存</li></ul></li></ul><img src="/2025/054_cmu_15618/MOESIF.webp" srcset="/img/loading.gif" lazyload><h2 id="inclusive-property"><a class="markdownIt-Anchor" href="#inclusive-property"></a> Inclusive property</h2><p>L1 L2 L3多级缓存</p><p>如果让所有L1缓存间和L2缓存间都interconnecting，那么效率低，所以让L2之间interconnect，并让L2 inclusive，即L2缓存中包含所有L1缓存，由L2控制L1的invalidate等</p><p><strong>Inclusive property</strong> of caches: all lines in closer (to processor) cache are also in farther (from processor) cache. e.g. contents of L1 are a subset of L2</p><p>如果单纯让L2比L1大，不能自动保证inclusion</p><p>需要让L1和L2相互交流，L2中维护一个“是否在L1中”的bit</p><img src="/2025/054_cmu_15618/multilevel-cache-inclusion.webp" srcset="/img/loading.gif" lazyload><h2 id="gpu-dont-have-cache-coherence"><a class="markdownIt-Anchor" href="#gpu-dont-have-cache-coherence"></a> GPU don’t have cache coherence</h2><p>每个cache都必须监听并对所有broadcast做出反应，这样interconnect开销会随着processor数量增长而增长</p><p>所以Nvidia GPU没有cache coherence，而是用atomic memory operation绕过L1 cache访问global memory</p><img src="/2025/054_cmu_15618/nvgpu-no-cache-coherence.webp" srcset="/img/loading.gif" lazyload><h1 id="lec-13-directory-based-cache-coherence"><a class="markdownIt-Anchor" href="#lec-13-directory-based-cache-coherence"></a> Lec 13. Directory-based Cache coherence</h1><p>上一节课讲Snooping</p><p>Snooping-based cache coherence需要依赖broadcast工作，每次cache miss时都要与其它所有cache通信</p><p>存在scalability问题</p><p>One possible solution: <strong>hierarcical snooping</strong>。缺点是root会成为瓶颈；延迟；不能用于不同的拓扑结构</p><h2 id="scalable-cache-coherence-using-directories"><a class="markdownIt-Anchor" href="#scalable-cache-coherence-using-directories"></a> Scalable cache coherence using <strong>Directories</strong></h2><p>在一个地方存directory，每条cache line的directory信息存储着所有cache中这条cache line的状态</p><p>用point-to-point messages代替broadcast来传数据</p><p>Directory中包含 <strong>dirty bit</strong> 和 <strong>presence bit</strong>, 第k个presence bit代表第k个processor是否有这条cache line</p><p>Distributed directory: directory与memory大小同步增长</p><img src="/2025/054_cmu_15618/simple-directories.webp" srcset="/img/loading.gif" lazyload><p>Example 1: <strong>Read miss to clean line</strong></p><p>Processor 0把位于1的内存数据读到了自己的local cache 里，对应的directory记录P0有值</p><img src="/2025/054_cmu_15618/directory-eg1.webp" srcset="/img/loading.gif" lazyload><p>Example 2: <strong>Read miss to dirty line</strong></p><p>本来P2的local cache中有dirty的数据</p><p>P0想读，发送read miss消息，P1告诉P0目前P2有dirty数据，P0收到后去向P2请求数据，P2将数据发给P0并将状态设置为shared，位置1的directory presence bit记录目前P0和P2有数据</p><img src="/2025/054_cmu_15618/directory-eg2.webp" srcset="/img/loading.gif" lazyload><p>Example 3: <strong>Write miss</strong></p><p>P0有一条cache line，将要写入这条cache line，因此先请求找出有哪些Processor目前有这条cache line（找出sharer ids）然后向它们（P1和P2）发送invalidate请求，收到P1和P2的ack之后代表它们两个已经invalidate，此时再进行写入内存操作。</p><img src="/2025/054_cmu_15618/directory-eg3.webp" srcset="/img/loading.gif" lazyload><p><strong>Advantage of directories</strong>:</p><ul><li>在read时，directory能直接告诉节点应该去问谁要数据，仅需要点对点通信：如果line is clean, 从home node要；如果line is dirty, 从owner node要。</li><li>在write时，directory告诉sharer id，工作量取决于有多少节点在共享数据。极端情况，如果所有cache都在共享数据，则需要与所有节点通信，像broadcast一样。</li></ul><h2 id="limited-pointer-schemes"><a class="markdownIt-Anchor" href="#limited-pointer-schemes"></a> Limited pointer schemes</h2><p>presense bit需要占用存储空间，会导致storage overhead</p><p>Reducing storage overhead</p><ul><li>increase cache line size: 让占比减小（M减小）</li><li>group multiple processors into a single directory node (让P减小)</li><li>除此之外还能使用 <strong>limited pointer scheme</strong> (降低P) 和 <strong>sparse directories</strong></li></ul><p><strong>Limited pointer schemes</strong>: 只存指针（指针=processor的id）</p><p>如果指针溢出，有几种不同的实际方法</p><ul><li>指针溢出时改为broadcast（添加一个additional bit代表指针不够用）</li><li>设置最大共享者数量，不允许超出，如果超出，老的sharer被移除</li><li>指针溢出时改为bit vector representation</li></ul><h2 id="sparse-directories"><a class="markdownIt-Anchor" href="#sparse-directories"></a> Sparse directories</h2><p>Key observation: majority of memory is NOT resident in cache.</p><p>Sparse directories只存一个指针，而在processor的cache line上存prev和next指针</p><img src="/2025/054_cmu_15618/sparse-directories.webp" srcset="/img/loading.gif" lazyload><p>优化：<strong>Intervention forwarding</strong>, <strong>Request forwarding</strong></p><hr><h1 id="一些基础知识"><a class="markdownIt-Anchor" href="#一些基础知识"></a> 一些基础知识</h1><h2 id="ispc"><a class="markdownIt-Anchor" href="#ispc"></a> ISPC</h2><p>ISPC代码调用时会生成多个program instances, 可以利用 <code>programCount</code> 和 <code>programIndex</code> 来获取instance总数和当前instance编号。</p><p><code>uniform</code> 表示在一个SIMD程序块中，变量对所有SIMD通道都是相同的值。仅仅是一种优化，不影响正确性(因为uniform变量只需要加载一次或执行一次，编译器可以做出优化，不加uniform可能造成不必要的重复计算)。</p><p>非uniform (<code>varying</code>) 表示变量在不同SIMD通道可能有不同的值。</p><p>所以说 <code>programCount</code> 是 uniform, <code>programIndex</code> 是 varying.</p><hr><p>ISPC可以通过tasks来实现多核加速，利用多线程。</p><p>Contrary to threads, tasks do not have execution context and they are only pieces of work. ISPC编译器接受tasks并自行决定启动多少个threads。</p><p>通常我们应该启动比cpu逻辑线程数更多的tasks数量，但也不要太多，否则会有scheduling的overhead。</p><p>task自带 <code>taskIndex</code>。</p><h2 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h2><p>host是CPU, device是GPU</p><p><code>__device__</code>: 在device上执行，只能在device中调用</p><p><code>__global__</code>: 在device上执行，只能在host中调用。叫做<strong>kernel</strong>，返回值必须是void</p><p><code>__host__</code>: 在host上执行且只能在host上调用</p><p><code>cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost)</code></p><hr><h3 id="threads-blocks-grids"><a class="markdownIt-Anchor" href="#threads-blocks-grids"></a> Threads, Blocks, Grids</h3><p>threads grouped into blocks</p><p>需要指明blocks的数量，和每个block中threads的数量。</p><p>假设n是总的threads数量, t是每个block中threads的数量。</p><p><code>KernelFunction&lt;&lt;&lt;ceil(n/t), t&gt;&gt;&gt;(args)</code></p><p>每一个thread都会运行同样的kernel，每一个thread由blockID和这个block中的threadID来标识。</p><hr><p>Example:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">vecAddKernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> i = threadId.x + blockDim.x * blockId.x;<br>    <span class="hljs-keyword">if</span> (i&lt;n) C[i] = A[i] + B[i];<br>&#125;<br><span class="hljs-type">void</span> <span class="hljs-title function_">vecAdd</span><span class="hljs-params">(<span class="hljs-type">float</span>* A, <span class="hljs-type">float</span>* B, <span class="hljs-type">float</span>* C, <span class="hljs-type">int</span> n)</span> &#123;<br>    <span class="hljs-type">int</span> size = n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-type">float</span> *d_A, *d_B, *d_C;<br>  <br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_A, size);<br>    cudaMemcpy(d_A, A, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_B, size);<br>    cudaMemcpy(d_B, B, size, cudaMemcpyHostToDevice);<br>    cudaMalloc((<span class="hljs-type">void</span> **) &amp;d_C, size);<br>  <br>    vecAddKernel&lt;&lt;&lt;<span class="hljs-built_in">ceil</span>(n/<span class="hljs-number">256</span>), <span class="hljs-number">256</span>&gt;&gt;&gt;(d_A, d_B, d_C, n);<br>    <br>  	cudaMemcpy(C, d_C, size, cudaMemcpyDeviceToHost);<br>    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);<br>&#125;<br></code></pre></td></tr></table></figure><p>注: 为什么<code>cudaMalloc</code>第一个参数是二级指针，而不直接使用返回值来赋值给指针？</p><p>因为 <code>cudaMalloc</code> 的返回值已经用来返回 <code>cudaError_t</code>。</p><hr><p>grid和blocks可以是1D, 2D, 3D的。上面这个例子是1D，所以是&quot;<code>.x</code>&quot;</p><p>2D的例子：假设要把一个WIDTH x WIDTH的矩阵P分成几块。</p><p>WIDTH=8, TILE_WIDTH为2的话，就是把8x8的矩阵分成16个小块(grid)，每一个小块大小是2x2(4个thread)。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c">dim3 <span class="hljs-title function_">dimGrid</span><span class="hljs-params">(WIDTH / TILE_WIDTH, WIDTH / TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>dim3 <span class="hljs-title function_">dimBlock</span><span class="hljs-params">(TILE_WIDTH, TILE_WIDTH, <span class="hljs-number">1</span>)</span>;<br>MatrixMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(args);<br></code></pre></td></tr></table></figure><p>每一个thread可以用以下方式来标识</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">row    = blockId.y * blockDim.y + threadId.y;<br>column = blockId.x * blockDim.x + threadId.x;<br></code></pre></td></tr></table></figure><hr><p>为什么用两层threads？因为组成多个grid的thread blocks比一个很大的单个thread block更好管理。</p><p>GPU有很多很多核心，核心group成SM(streaming multiprocessors)，每一组SM有自己的内存和调度。</p><p>GPU不同时启动所有100万个threads，而是把大约1000个thread装进一个block里，并分发给SM。</p><p>assign给SM的thread block会使用SM的资源（寄存器和共享内存）。这些资源已经pre-allocated，且由于寄存器数量很多，在切换threads时不需要register flush。</p><hr><p>不同的block可以用任何顺序运行，因此不能assume block2在block1之后运行。如果真的要这么做，需要放在不同的kernel里（启动kernel比较耗资源）</p><p>同一个block中的thread可以使用 <code>__syncthreads()</code> 来做barrier synchronization。</p><p>但是通常不建议使用 <code>__syncthreads()</code></p><hr><p>如何选择合适的block size？</p><ul><li>Consideration 1: hardware constraints<ul><li>例如：每一个SM分配小于1536个thread，小于8个block；每一个block小于512个thread</li></ul></li><li>Consideration 2: complexity of each thread</li><li>Consideration 3: thread work imbalance.</li></ul><hr><p>GPU memory</p><p>Global memory很慢，所以同时运行大量线程，线程因为内存IO卡住的时候切换其它线程，这是massive multi-threading (MMT).</p><p>这样总的throughput很高，即使每个thread的延迟也很高。</p><p>每个SM有自己的scheduler，每个SM存储了所有thread的context(PC, reg等)，所以SM内能做到零开销线程切换。同时，SM scheduler有一个scoreboard追踪哪些thread是blocked/unblocked，所以SM有大约30个核但可以运行大约1000个线程。</p><hr><p>Tiled MM是一种进行矩阵乘法 内存友好的方法。</p><p>CUDA类型关键词</p><ul><li><code>__device__ __shared__</code> memory: shared; scope: block; lifetime: block</li><li><code>__device__</code> memory: global; scope: grid; lifetime: application</li><li><code>__device__ __constant__</code> memory: constant; scope: grid; lifetime: application</li></ul><hr><p>Race conditions:</p><p>CUDA中难以实现mutex，而且包含critical sections的代码在GPU上本来就运行得不好。</p><p>CUDA中有一些原子操作，可以在global或shared memory变量上操作</p><ul><li><code>int atomicInc(int *addr)</code>: 加一，返回旧值</li><li><code>int atomicAdd(int *addr, int val)</code>: 加val, 返回旧值</li><li><code>int atomicMax(int *addr, int val)</code>: 让*addr=max(*addr, val) 并返回旧值</li><li><code>int atomicExch(int *addr1, int val)</code>: set</li><li><code>int atomicCAS(int *addr, old, new)</code>: Compare and swap.<ul><li><code>if (*addr == old) *addr = new;</code></li></ul></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/Lecture-Notes/" class="category-chain-item">Lecture Notes</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/C/" class="print-no-link">#C</a> <a href="/tags/Lecture-Notes/" class="print-no-link">#Lecture Notes</a> <a href="/tags/CUDA/" class="print-no-link">#CUDA</a></div></div><div class="license-box my-3"><div class="license-title"><div>[Lecture Notes] CMU 15-618 Parallel Computer Architecture &amp; Programming</div><div>https://www.billhu.us/2025/054_cmu_15618/</div></div><div class="license-meta"><div class="license-meta-item"><div>Author</div><div>Bill Hu</div></div><div class="license-meta-item license-meta-date"><div>Posted on</div><div>January 7, 2025</div></div><div class="license-meta-item"><div>Licensed under</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2025/057_cmu_17514/" title="[Lecture Notes] CMU 17-514 Software Construction"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">[Lecture Notes] CMU 17-514 Software Construction</span> <span class="visible-mobile">Previous</span></a></article><article class="post-next col-6"><a href="/2024/051-cmu-15513/" title="[Lecture Notes] CMU 15-513 Intro to Computer Systems"><span class="hidden-mobile">[Lecture Notes] CMU 15-513 Intro to Computer Systems</span> <span class="visible-mobile">Next</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Table of Contents</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Search</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Keyword</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <span>With </span><a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <i class="iconfont icon-love"></i> <a href="/about" target="_blank" rel="nofollow noopener"><span>Bill Hu</span></a><div style="font-size:.85rem"><span id="timeDate">Loading days...</span> <span id="times">Loading time...</span><script src="/js/duration.min.js"></script></div></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,t=t.getElementById("subtitle");t&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog works best with JavaScript enabled</div></noscript></body></html>